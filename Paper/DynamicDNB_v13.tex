%\documentclass[12pt]{article}
%\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
%\usepackage{t1enc}
%\usepackage{fancyhdr}
%\usepackage[T1]{fontenc} % eurpai fontkszlet, ,, >> << is mukdik
%\frenchspacing % mondatok kzti normlis szkz (ez ktelez?)
%\usepackage[pdftex]{graphicx}
%\usepackage{epstopdf}

\documentclass[12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}


%\usepackage{placeins}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{natbib}
\usepackage{color}
\usepackage{amsmath}
%\usepackage{bbm}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75} %should be lower than topfraction from Jakob's Latex tricks :)
\usepackage{setspace} %spacing
\onehalfspacing
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.8cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage[scriptsize,bf]{caption}
\usepackage{float}
\restylefloat{figure}
%\usepackage[colorlinks]{hyperref}
\usepackage{subfigure}
%\usepackage{booktabs}
%\usepackage{commath}
\newcommand{\dd}{{\rm d}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
%\usepackage{accents}
%\newcommand\munderbar[1]{\underaccent{\bar}{#1}}
\newcommand\munderbar[1]{\underbrace{#1}}
\newcommand\mathbbm[1]{\mathbf{#1}}
\begin{document}

\title{\Huge{Bayesian Dynamic Modeling of \\ High-Frequency Integer Price Changes}\footnote{Corresponding author:
S.J. Koopman, Department of Econometrics, VU Amsterdam, De Boelelaan 1105, 1081 HV Amsterdam,
The Netherlands. Emails : \texttt{barra.istvan@gmail.com} \ \texttt{s.j.koopman@vu.nl} \
IB thanks Dutch National Science Foundation (NWO) for financial support.
SJK acknowledges support from CREATES, Center for Research in Econometric Analysis of Time Series (DNRF78), funded by the Danish National Research Foundation.
We are indebted to Lennart Hoogerheide, Rutger Lit, Andr\'e Lucas and Mike Pitt for
their help and support in this research project and to Rudolf Fr\"{u}hwirth for providing the C code for auxiliary mixture sampling. This version: \today} 
%\\
%\vspace{0.5cm}
%\Large PRELIMINARY AND INCOMPLETE
}



\author{\textit{Istv\'an Barra}$^{(a)}$ and \textit{Siem Jan Koopman} $^{(a,b)}$
\\
$^{(a)}$ \small Vrije Universiteit Amsterdam and Tinbergen Institute\\
$^{(b)}$ \small CREATES, Aarhus University \\
\date{ }
}

\maketitle


%\vspace{-1.0cm}
\begin{abstract}

We investigate high-frequency volatility models for
analyzing
intra-day tick by tick stock price changes using Bayesian estimation
procedures.
Our key interest is the extraction of intra-day volatility patterns
from high-frequency integer price changes.
%We explicitly
We account for the discrete nature of the data via two
different approaches: ordered probit models and discrete distributions.
%,
%both with heavy tail generalizations.
%In these approaches w
We allow for stochastic volatility by modeling
the variance as a stochastic function of time, with intra-day periodic patterns.
We consider distributions with heavy tails to address
occurrences of jumps in tick by tick discrete prices changes.
In particular, we introduce a dynamic version of the negative binomial difference model
with stochastic volatility.
%The model allows for the discreteness of the
%observed prices, heavy tails of tick by tick price changes and intra-day patterns
%of volatility in a unified manner.
For each model we develop a Markov chain Monte Carlo estimation method
that takes advantage of auxiliary mixture representations to
facilitate the numerical implementation.
This new modeling framework is illustrated 
%We illustrate our modeling approaches
by means of tick by tick data for
several stocks from the  NYSE and for different periods.
Different models are compared with each other
%We make comparisons between the modeling approaches.
%For example,
based on predictive likelihoods.
We find evidence in favour of our preferred dynamic
negative binomial difference model.\newline
\text{ }
\newline

\noindent \textit{Keywords}:
Bayesian inference;
discrete distributions;
high-frequency dynamics;
Markov chain Monte Carlo;
stochastic volatility.
   
\end{abstract}

\section{Introduction}\label{c_sec:intro}

High-frequency price changes observed at stock, futures and
commodity markets can typically not be regarded as continuous variables.
In most electronic markets, the smallest possible price difference
is set by the regulator or the trading platform.
Here we develop and investigate dynamic models for
high-frequency integer price changes that take the discreteness
of prices into account.
We explore the dynamic properties of
%irregularly spaced
integer time series observations.
In particular, we are interested in the stochastic
volatility dynamics of price changes
within intra-daily time intervals.
This information can be used for the timely identification of
changes in volatility and to obtain more accurate estimates
of integrated volatility.

In the current literature on high-frequency returns,
price discreteness is typically neglected.
However, the discreteness can have an impact
on the distribution of price changes and on its volatility; see, for example,
Security and Exchange Commission Report (2012), \nocite{SEC2012}
\citet{CWWanNess2004} and \citet{RonenWeaver2001}.
Those assets that have prices with a spread of almost always equal to one tick
are defined as large tick assets; see, \citet{EislerBK2011}.
These large tick assets are especially affected by the discreteness
through the effect of different quoting strategies on these assets;
see the discussions in \citet{ChordiaS1995} and \citet{CordellaFoucault1999}.
Also the effect of liquidity on large tick assets can be substantial as
it is documented by \citet{OHaraSaarZhong2014} and \citet{YeYao2014}.
Many large tick assets exist on most US exchange markets as the
tick size is set to only one penny for stocks with a
price greater than 1$\$$ by the Security and Exchange Commission
in Rule 612 of the Regulation National Market System.
Hence almost all low price stocks are large tick assets.
Moreover, many future contracts are not decimalized for example, five-years U.S Treasury Note futures and EUR/USD futures
fall into this category (see 
\citet{DayriRosenbaum2013}).


   


%The empirical relevance of studying the discrete price changes and the corresponding
%discrete bid-ask spreads is emphasized in \citet{Harris1994}.
%The monitoring of minimum price variations can be helped by high-frequency updates
%of volatility estimates as bid-ask spreads and quotation sizes are affected by
%volatility changes.
%Furthermore, high-frequency volatility also reflect the overall fluctuations of order books and hence
%it can facilitate the automatic identification of price jumps;
%see \citet{EislerBK2011}.
%In a recent paper by \citet{YeYao2014}, a causal effect is established for the
%price change in ticks on the role of high-frequency traders with respect to
%the non-high-frequency traders; the magnitude of price changes (volatility) has a clear impact on speed competition.
%The authors conclude that high intra-day volatility decreases liquidity overall but it
%increases liquidity for the high-frequency traders.
%Hence the accurate extraction of high-frequency volatility changes will
%provide further insights into micro-structure behaviour in financial markets.

The relevance of discreteness and its effect on the analysis of
price changes have been the motivation to develop models that account for
integer prices.
Similar to the case of continuous returns, we are primarily interested in
the extraction of volatility from discrete price changes.
We consider different dynamic model specifications
for the high-frequency integer price changes with a focus on the modeling
and extraction of stochastic volatility.
We have encountered the studies of
\citet{MullerCzado2006} and \citet{Stefanos2015}
who propose ordered probit models with time-varying variance specifications.
We adopt their modeling approaches as a reference and also use their
treatments of Bayesian estimation.
The main novelty of our study is the specification of 
a new model for tick by tick price changes based on
the discrete negative binomial distribution which we shall
refer to shortly as the $\Delta$NB distribution.
The properties of this distribution are explored in detail in our study.
In particular, the heavy tail properties are emphasized.
In our analysis, we adopt the $\Delta$NB distribution conditional
on a Gaussian latent state vector process which represent the
components of the stochastic volatility process.
The volatility process accounts for the periodic pattern in
high-frequency volatility due to intra-day seasonal effects such as the
opening, lunch and closing hours.
Our Bayesian modeling approach provides a flexible and unified framework
to fit the observed tick by tick price changes.
The $\Delta$NB properties closely mimic
the empirical stylized properties of trade by trade price changes.
Hence we will argue that the $\Delta$NB model with stochastic volatility
is an attractive alternative to models based on the Skellam
distribution as suggested earlier; see \citet{KoopmanLucasLit2014}.
We further decompose the unobserved log volatility
into intra-daily periodic and transient volatility components.
We propose a Bayesian estimation procedure using standard
Gibbs sampling methods.
Our procedure is based on data augmentation and auxiliary mixtures;
it extends the auxiliary mixture sampling procedure proposed by
\citet{SFSWagner2006} and \citet{SFSFHeldRue2009}.
The procedures are implemented in a computationally efficient manner.
%which has allowed us to carry out a Monte Carlo study to validate
%the estimation procedure.


In our empirical study we consider
six stocks from the NYSE in a volatile week in October 2008
and a calmer week in April 2010.
We compare the in-sample and out-of-sample fits of four different
model specifications:
ordered probit model based on the normal and Student's $t$ distributions,
the Skellam distribution and the $\Delta$NB model.
%
%each approach is represented by two models: a thin taild and a
%fat tailed distribution specification
%
%
%of the
%two different approaches (ordered probit and discrete distributions)
%while each approach is represented by two models:
%with a Gaussian and a t-distribution specification.
We compare the models in terms of Bayesian information criterion
and predictive likelihoods. We find that the $\Delta$NB model
is favoured for stocks with a relatively low tick size and
in periods of more volatility.


%  The $\Delta$NB distribution is introduced by \citet{BNPollardShephard2012} as the difference of to negative binomial random variables. $\Delta$Negative Binomial distribution is an integer valued distribution, hence it is a natural candidate for modelling financial tick returns, moreover it has fatter tails then the Skellam distribution which makes it an attractive alternative.







%price discretness  vs dark markets 

Our study is related to different strands in the econometrics literature.
Modeling discrete price changes with static Skellam and
$\Delta$NB distributions has been introduced by \citet{AlzaidOmair2010}
and \citet{BNPollardShephard2012}.
The dynamic specification of the Skellam distribution
and its (non-Bayesian) statistical treatment have been
explored by \citet{KoopmanLucasLit2014}.
Furthermore, our study is related to Bayesian treatments of
stochastic volatility models for continuous returns; see, for example,
\citet{ChibNardariShephard2002},
\citet{KimShephardChib1998},
\citet{OmoriChibShephardNakajima2007} and, more recently,
\citet{JohannesStroud2014}.
We extend this literature on trade by trade price changes
by explicitly accounting for prices discreteness and
heavy tails of the tick by tick return distribution.
These extensions are explored in other contexts in
\citet{Engle2000}, \citet{CzadoHaug2010},
\citet{DahlhausNeddermeyer2014} and \citet{RydbergShephard2003}.

The remainder is organized as follows.
In Section \ref{c_sec:model} we review different dynamic model
specifications for high-frequency integer price changes.
We give most attention to the
introduction of the dynamic $\Delta$NB distribution.
Section \ref{c_sec:estimation} develops a Bayesian estimation procedure
based on Gibbs sampling, mainly for the $\Delta$NB case of which the
Skellam is a special case.
In Section \ref{c_sec:empirical} we present the details of our
empirical study including a description of our dataset,
the data cleaning procedure, the presentation of our estimation
results and a discussion of our overall empirical findings.
Section \ref{c_sec:concl} concludes.

%Prices are observed on a discrete grid, this  has  statistical and financial consequences  
%\begin{itemize}
%\item  large tick size stock Eisler Harris
%\item Mao Ye consequences on liquidity and market efficiency penny 
%\item there are markets where the tick size is quite large  
%\end{itemize}
%We don't know too much about the distribution of price changes (why price changes?)
%Recently there car considerable interest in modelling discrete price moves
%What we do
%\begin{itemize}
%\item introduce the dynamic negative binomial difference model
%\item we propose a Bayesian estimation procedure
%\item in an empirical illustration we compare the negative binomial model with alternative models 
%\end{itemize}


\section{Dynamic models for discrete price changes}
\label{c_sec:model}

In this section we first review the modeling of
integer valued variables using ordered probit models
based on normal and Student's $t$ distributions
with stochastic volatility.
Next we introduce the dynamic negative binomial difference
($\Delta$NB) model with stochastic volatility and discuss
its features. The dynamic Skellam model is a special case of
$\Delta$NB.

\subsection{Ordered normal stochastic volatility model}\label{c_sec:ordern}

In econometrics, the ordered probit model is typically used for
the modeling of ordinal variables.
But we can also adopt the ordered probit model in a natural way for the
modeling of discrete price changes. In this approach we effectively
round a realization from a continuous distribution to its nearest integer.
The continuous distribution can be subject to
stochastic volatility; this extension is relatively straightforward.
Let $r^{*}_{t}$ be the continuous return which
is rounded to $r_{t}=k$ when $r^{*}_{t} \in [k-0.5,k+0.5)$.
We observe $r_t$ and we regard $r^{*}_{t}$ as a latent variable.
By neglecting the discreteness of $r_{t}$ during the estimation
procedure, we clearly would distort the measurement of the scaling
or variation of $r_t^*$.
Therefore we need to take account of the rounding of $r_{t}$ by specifying
an ordered probit model with rounding thresholds $[k-0.5,k+0.5)$.
We assume that the underlying distribution for $r^{*}_{t}$ is subject to
stochastic volatility.
We obtain the following specification
\begin{equation}
r_{t} =k, \quad \text{with probability} \ \Phi\left(\frac{k+0.5}{  \exp(h_t/2)}\right)- \Phi\left(\frac{k-0.5}{ \exp(h_t/2)}\right),  \quad \text{for} \ k \in \mathbb{Z},
\label{eq:probitn}
\end{equation}
for $t=1,\ldots,T$,
where $h_{t}$ is the logarithm of the time varying stochastic variance
for the standard normal distribution with cumulative density function
$\Phi(\cdot)$ for the latent variable $r^{*}_{t}$.
Similar ordered probit specifications with
stochastic volatility are introduced by \citet{MullerCzado2006} and
\citet{Stefanos2015}. The dynamic model specification for $h_{t}$ is given by
%\begin{eqnarray}
\begin{equation}
h_{t} = \mu_{h} +x_{t}, \qquad
x_{t+1} = \varphi x_{t} + \eta_{t}, \qquad
\eta_{t} \sim \mathcal{N}\left(0, \sigma^{2}_{\eta}\right),
\label{eq:logsv}
\end{equation}
%\end{eqnarray}
for $t=1,\ldots,T$,
where $\mu_{h}$ is the unconditional mean of the log volatility of
the continuous returns,
$x_{t}$ is a zero mean autoregressive process (AR) of order one, that is AR(1),
with $\varphi$ as the persistence parameter
for the log volatility process and
$\sigma^{2}_{\eta}$ as the variance of the Gaussian disturbance term
$\eta _{t}$.
The mean $\mu _h$ represents the daily log volatility and
the autoregressive process $x_t$ captures the changes in log volatility due to
firm specific or market information experienced during the day.
The latent variable $x_t$ is specified as an AR(1) process with zero mean;
this restriction is enforced to allow for the identification of $\mu _h$.

The basic model specification \eqref{eq:probitn} - \eqref{eq:logsv}
accounts for the discreteness of the prices
via the ordered probit specification and
for intra-day volatility clustering via the possibly persistent dynamic process of $x_{t}$.
The model captures the salient empirical features of high-frequency
trade by trade price changes.
Another stylized fact of intra-day price changes is the seasonality
pattern in the volatility process.
In particular, the volatility at the opening minutes of the trading day is high,
during the lunch-hour it is lowest, and at the closing minutes it is increasing somewhat.
We can account for such an intra-day volatility pattern by including
a cubic spline in the log volatility specification, that is
\begin{equation}
\label{eq:logsvspl}
h_t = \mu_h + s_t +x_t ,
\qquad
E(s_t) = 0 ,
%s_t &=& w_t \beta \\
%x_{t+1} &=& \phi x_t + \eta_t \quad \eta_t \sim \mathcal{N}\left(0, \sigma^{2}\right),
\end{equation} 
where $s_t$ is a normalized spline function with its unconditional mean equal to zero.
This specification implies a decomposition of the variance
of the continuous return distribution $r^{*}_t$  into a
deterministic daily seasonal pattern $s_t$ and a stochastically
time varying signal $x_t$.
%The daily pattern of volatility usually associated to frequent
%trading during the beginning of the day and lower activity during lunch.
We use intradaily cubic spline function,
constructed from $K+1$ piecewise cubic polynomials, to capture the daily seasonality. 
We adopt the representation of \citet{Poirier1973} where
the periodic cubic spline $s_t$ is based on $K$ knots and the
regression equation
\begin{equation}
\label{eq:spline}
s_t = w_t \beta 
%+ \varepsilon_t,\quad \varepsilon_t \sim \mathcal{N}(0, \sigma^{2}_{\varepsilon})
\end{equation}
where $w_t$ is a $1 \times K$ weight vector and $\beta$ is a $K \times 1$
vector which contains values of the spline function at the $K$ knots.
Further details about the spline and the Poirier representation
are presented in Appendix \ref{c_sec:spline_appendix}.
For alternative treatments of intra-daily seasonality, we refer to  
\citet{Bos2008}, \citet{JohannesStroud2014} and
\citet{WeinbergBrownStroud2007}.



%Some comments about the above model specification are in order.
The model can be modified and extended in several ways.
First, we can account for the
market microstructure noise observed in tick by tick returns (see for example,
\citet{AitSahaliaMyklandZhang2011} and \citet{GriffinOomen2008}) by
including an autoregressive moving average (ARMA) process
in the specification of the mean of $r^{*}_{t}$.
In a similar way, we can facilitate the incorporation of explanatory
variables such as market imbalance which can also have predictive power.
%We do not entertain this possibility in this paper,
%but we are currently studying these possibilities.
Second, to include predetermined announcement effects, we can
include regression effects in
the specification as proposed in \citet{JohannesStroud2014}.
Third, it is possible that the unconditional mean $\mu _h$ of the volatility of
price changes is time varying.
For example, we may expect that for larger price stocks the volatility
is higher and therefore the volatility is not properly scaled when the price
has changed.
The time-varying conditional mean of the volatility can be easily
incorporated in the model, by specifying a random walk dynamics
for $\mu_{h}$, which would allow for smooth changes in the mean over time.
For our current purposes below we can rely on the specification as given by
equation \eqref{eq:logsvspl}.


\subsection{Ordered $t$ stochastic volatility model}\label{c_sec:ordert}

It is well documented in the financial econometrics literature that
asset prices are subject to jumps; see, for example, \citet{AitSahaliaJacodLi2012}.
However, the ordered normal specification, as we have introduced it above,
does not deliver sufficiently heavy tails
in its asset price distribution to accommodate the jumps that are
typically observed in high-frequency returns.
To account for the jumps more appropriately,
we can consider a heavy tailed distribution instead of the normal distribution.
In this way we can assign probability mass to the infrequently large jumps
in asset returns.
An obvious choice for a heavy tailed distribution is the Student's $t$-distribution
which would imply the following specification,
\begin{equation}
\label{eq:probitt}
r_{t} =k, \quad \text{with probability} \
\mathcal{T}\left(\frac{k+0.5}{  \exp(h_t/2)} ,\nu \right)-
\mathcal{T}\left(\frac{k-0.5}{ \exp(h_t/2)},\nu\right),
\quad \text{for} \ k \in \mathbb{Z},
%h_t &=& \mu_h + s_t +x_t \\
%s_t &=& w_t \beta \\
%x_{t+1} = \phi x_t + \eta_t \quad \eta_t \sim \mathcal{N}\left(0, \sigma^{2}\right),
\end{equation}
which effectively replaces model equation \eqref{eq:probitn},
where $\mathcal{T}\left(\cdot,\nu \right)$ is the
cumulative density function of the
Student's $t$-distribution with $\nu$ as the degrees of freedom parameter.
The model specification for $h_t$ is provided by equation \eqref{eq:logsv} or
\eqref{eq:logsvspl}.

The parameter vector of this model specification is denoted by $\psi$ and
includes the degrees of freedom $\nu$, the unconditional mean of log volatility
$\mu _h$, the volatility persistence coefficient $\varphi$, the variance
of the log volatility disturbance $\sigma^{2}_{\eta}$, and the
unknown vector $\beta$ in \eqref{eq:spline} with values of the spline at its knot positions.
In case of the normal ordered probit specification, we can rely on the same
parameters but without $\nu$.
The estimation procedure for these unknown parameters in the
ordered probit model specifications are carried out by standard Baysian simulation
methods for which the details are provided in Appendix \ref{c_sec:tapp}.


\subsection{Dynamic $\Delta$NB model}

Positive integer variables can alternatively be modeled directly via discrete distributions such
as the Poisson or the negative binomial, see \cite{JohnsonKempKotz2005}.
These well-known distributions only provide support to positive integers.
When modeling price differences, we also need to allow for negative integers.
For example, in this case, the Skellam distribution can be considered,
see \cite{Skellam(46)}.
The specification of these distributions can be extended to
stochastic volatility model straightforwardly. However, the analysis and
estimation based on such models are more intricate.
In this context, \citet{AlzaidOmair2010}  advocates the use of the
Skellam distribution based on the difference of two Poisson random variables.
\citet{BNPollardShephard2012} introduces the negative binomial
difference ($\Delta$NB) distribution which have fatter tails
compared to the Skellam distribution. 
Next we review the $\Delta$NB distribution and its properties.
We further introduce a dynamic version of the $\Delta$NB model from which
the dynamic Skellam model is a special case.

The $\Delta$NB distribution is implied by the construction of
the difference of two negative binomial random variables
which we denote by $ NB^{+}$ and $ NB^{-}$ where the variables
have number of failures $\lambda^{+}$ and $\lambda^{-}$, respectively,
and failure rates $\nu^{+}$ and $\nu^{-}$, respectively.
We denote the $\Delta$NB variable as the random variable
$R$ and is simply defined as
\begin{equation}
R=NB^{+}-NB^{-}.
\end{equation}
We then assume that $R$ is distributed as 
\begin{equation}
R \sim \textrm{$\Delta$NB}(\lambda^{+}, \nu^{+} ,\lambda^{-}, \nu^{-}) , 
\end{equation}
where $\Delta$NB is the difference negative binomial distribution 
with probability mass function given by
%\begin{equation}
%\begin{cases}\left(\frac{\nu^{+}}{ \lambda^{+}+\nu^{+}}\right)^{\nu^{+}}\left(\frac{\nu^{-}}{ \lambda^{-}+\nu^{-}}\right)^{\nu^{-}} \left(\frac{\lambda^{+}}{ \lambda^{+}+\nu^{+}}\right)^r \frac{(\nu^{+})_{r} }{r!}  F\left(\nu^{+}+r, \nu^{-}, r+1; \left(\frac{\lambda^{+}}{ \lambda^{+}+\nu^{+}}\right)\left(\frac{\lambda^{-}}{ \lambda^{-}+\nu^{-}}\right)\right) & \text{if} \quad r \geq 0 \\
%\left(\frac{\nu^{+}}{ \lambda^{+}+\nu^{+}}\right)^{\nu^{+}}\left(\frac{\nu^{-}}{ \lambda^{-}+\nu^{-}}\right)^{\nu^{-}} \left(\frac{\lambda^{-}}{ \lambda^{-}+\nu^{-}}\right)^{-r} \frac{(\nu^{-})_{r} }{r!}  F\left(\nu^{+}, \nu^{-}-r, -r+1; \left(\frac{\lambda^{+}}{ \lambda^{+}+\nu^{+}}\right)\left(\frac{\lambda^{-}}{ \lambda^{-}+\nu^{-}}\right)\right) & \text{if}  \quad r \leq 0 
%\end{cases}
%\end{equation}

%\begin{small}
\begin{equation}
f_{\Delta NB}(r; \lambda^{+}, \nu^{+} ,\lambda^{-}, \nu^{-})= m \times
\begin{cases}
d^{+} \times F\left(\nu^{+}+r, \nu^{-}, r+1; \, \tilde{\lambda}^{+}\tilde{\lambda}^{-}\right), & \text{if} \quad r \geq 0 ,\\
d^{-} \times F\left(\nu^{+}, \nu^{-}-r, -r+1;\, \tilde{\lambda}^{+}\tilde{\lambda}^{-}\right), & \text{if}  \quad r < 0 , \nonumber
\end{cases}
\end{equation}
%\end{small}
where
$m = \left(\tilde{\nu}^{+}\right)^{\nu^{+}}\left(\tilde{\nu}^{-}\right)^{\nu^{-}}$,
$d^{[s]} = (\tilde{\lambda}^{[s]})^r (\nu^{[s]})_{r}\, / \, r!$,
\[
%\tilde{\nu}^{+} = \frac{\nu^{+}}{ \lambda^{+}+\nu^{+}} , \quad
%\tilde{\nu}^{-} = \frac{\nu^{-}}{ \lambda^{-}+\nu^{-}} , \quad
\tilde{\nu}^{[s]} = \frac{\nu^{[s]}}{ \lambda^{[s]}+\nu^{[s]}} , \qquad
\tilde{\lambda}^{[s]} = \frac{\lambda^{[s]}}{ \lambda^{[s]}+\nu^{[s]}} , \quad
%\tilde{\lambda}^{+} = \frac{\lambda^{+}}{ \lambda^{+}+\nu^{+}} , \quad
%\tilde{\lambda}^{-} = \frac{\lambda^{-}}{ \lambda^{-}+\nu^{-}} ,
\]
for $[s]=+,-$,
and with the hypergeometric function 
\[
F(a, b, c; z) =
\sum \limits_{n=0}^{\infty}
\frac{ (a)_{n} (b)_{n}}{(c)_{n}} \, \frac{z^n}{n!} ,
\]
where $(x)_{n}$ is the Pochhammer symbol of falling factorial
and is defined as
\begin{equation}
(x)_{n} =x (x-1)(x-2) \cdots(x-n+1)=\frac{\Gamma(x+1)}{\Gamma(x-n+1)}.
\end{equation}
More details about the $\Delta$NB distribution, its probability mass
function and properties are provided by \citet{BNPollardShephard2012}.
For example,
the $\Delta$NB distribution has the following first and second moments
\begin{equation}
\textrm{E}(R)= \lambda^{+} -\lambda^{-} ,  \qquad
%\end{equation}
%\begin{equation}
\textrm{Var}(R)= \lambda^{+} \left(1+\frac{\lambda^{+}}{\nu^{+}}\right)+\lambda^{-} \left(1+\frac{\lambda^{-}}{\nu^{-}}\right).
\end{equation}
%
The variables $\nu ^{+}$, $\nu ^{-}$, $\lambda ^{+}$ and
$\lambda ^{-}$ are treated typically as unknown coefficients.

An important special case 
is the zero mean $\Delta$NB distribution which
is obtained when $\lambda=\lambda^{+}= \lambda^{-}$ and $\nu=\nu^{+}= \nu^{-}$.
The probability mass function for the corresponding random variable
$R$ is given by
\begin{equation*}
f_0(r; \lambda, \nu )=\left(\frac{\nu}{ \lambda+\nu}\right)^{2\nu}\left(\frac{\lambda}{ \lambda+\nu}\right)^{|r|} \frac{\Gamma(\nu+|r|) }{\Gamma(\nu)\Gamma(|r|+1)} F\left(\nu+|r|, \nu, |r|+1; \left(\frac{\lambda}{ \lambda+\nu}\right)^2\right) .
\end{equation*}
In this case we have obtained a zero mean random variable $R$ with
its variance given by
%\begin{equation}
%\textrm{E}(R)= 0
%\end{equation}
\begin{equation}
\textrm{Var}(R)= 2\lambda \left(1+\frac{\lambda}{\nu}\right).
\end{equation}
We denote the distribution for the zero mean random variable $R$ by
$\Delta$NB$(\lambda, \nu)$. This random variable $R$ can alternatively be
considered as being generated from a compound Poisson process, that is
\begin{equation}
R=\sum \limits_{i=1}^{N} M_i ,
\end{equation}
where random variable $N$ is generated by
the Poisson distribution with intensity 
\begin{equation}
\lambda \, \times \, (z_1 + z_2) , \qquad z_1,\, z_2 \sim \text{Ga}(\nu,\nu) 
\label{c_eq:gammapo}
\end{equation}
%Istvan: if intensity should be the function $\lambda(z_1 +z_2)$ then
%we should explicitly specify it
with $\text{Ga}(\nu,\nu)$ being the Gamma distibution, having its shape and
scale both equal to $\nu$,
and where indicator variable $M_i$ is generated as
\begin{equation}
M_i =
\begin{cases}
 1, & \text{with probability} \quad P(M_i =  1)= z_1 \, / \, (z_1 + z_2), \\
-1, & \text{with probability} \quad P(M_i = -1)= z_2 \, / \, (z_1 + z_2).
\end{cases}
\end{equation}
We will use this representation of a zero mean $\Delta$NB variable
for developments below.

In the empirical analyses of this study,
we adopt the zero inflated versions of the $\Delta$NB distributions,
because empirically we observe a clear overrepresentation of
trade by trade price changes that are equal to zero.
The number of these zero price changes are especially high for the more
liquid stocks. This is due to the available volumes on best bid and ask prices
which are relatively much higher. Hence the price impact of one trade is much
lower as a result. The zero inflated version is accomplished by the specification
of the random variable $R_{0}$ as
\begin{eqnarray*}
r_{0} &=& \begin{cases} r, & \text{with probability} \quad (1-\gamma)f_{\Delta\text{NB}}(r; \lambda^{+}, \nu^{+} ,\lambda^{-}, \nu^{-}), \\
0, & \text{with probability} \quad  \gamma + (1-\gamma)  f_{\Delta\text{NB}}(0; \lambda^{+}, \nu^{+} ,\lambda^{-}, \nu^{-}), \end{cases}
\end{eqnarray*}
where $f_{\Delta\text{NB}}(r; \lambda^{+}, \nu^{+} ,\lambda^{-}, \nu^{-})$
is the probability mass function for $r$
and $0<\gamma<1$ is treated as a fixed and unknown coefficient. We denote the zero inflated $\Delta$NB probability mass function with $f_0$.
%In the case of a zero mean $\Delta$NB, we can replace $f_{\Delta\text{NB}}$ by $f_0$.


The dynamic specifications of the $\Delta$NB distributions can be obtained
by letting the variables $\nu ^{[s]}$ and/or $\lambda ^{[s]}$ be time-varying
random variables, for $[s]=+,-$.
We opt to have a time-varying $\lambda ^{[s]}$ 
since it is more natural for an intensity than a degrees of freedom parameter to
vary over time.  We restrict our analysis to the  zero inflated zero mean $\Delta$NB distribution $f_{0}(r_{t}; \lambda_t,  \nu)$ which means we use zero inflation and we assume that the degree of freedom parameters for positive and negative price changes are the same and $\lambda_t=\lambda ^{+}_t=\lambda ^{-}_t$. 
Taking the above considerations into account, the dynamic $\Delta$NB model
can be specified as above but with
\[
\lambda_t = \exp(h_t),
\]
where $h_t$ is specified as in equation \eqref{eq:logsv} or \eqref{eq:logsvspl}.
Hence this dynamic specification is similar to the one
described in Section \ref{c_sec:ordern} for the ordered normal specification.
%A similar dynamic specification can be adopted for the zero inflated $\Delta$NB distribution $f_0$.
%
%\begin{eqnarray}
%y_t &=& \begin{cases} r_t & \text{with} \quad (1-\gamma)f_{\Delta\text{NB}}(r_t; \lambda_t, \nu ) \\
%0 & \text{with} \quad  \gamma + (1-\gamma)  f_{\Delta\text{NB}}(0; \lambda_t, \nu ) \end{cases} \\
%\log \lambda_t &=&h_t = \mu_h + s_t +x_t \label{c_eq:vol}\\
%s_t &=& w_t \beta \\
%x_{t+1} &=& \phi x_t + \eta_t \quad \eta_t \sim \mathcal{N}\left(0, \sigma^{2}_{\eta}\right),
%\end{eqnarray}
%where $\gamma$ is the zero inflation parameter.

\subsection{Dynamic Skellam model}

The dynamic $\Delta$NB model embeds the dynamic Skellam model as
considered by \citet{KoopmanLucasLit2014}. It is obtained as the limiting case of
letting $\nu$ go to infinity, that is $\nu \rightarrow \infty$;
for a derivation and further details, see Appendix \ref{c_sec:nb}.



\section{Bayesian estimation procedures}
\label{c_sec:estimation}

Bayesian estimation procedures for the ordered normal and ordered
Student's $t$ stochastic volatility models are discussed by
\citet{MullerCzado2006} and \citet{Stefanos2015};
their procedures, with some details, are presented in Appendix \ref{c_sec:tapp}.

Here we develop a Bayesian estimation procedure for observations
$y_t$, with $t=1,\ldots,T$, coming from
the dynamic $\Delta$NB model. We provide the details of the procedure and
discuss its computational implementation.
Our reference dynamic $\Delta$NB model is given by
\begin{eqnarray}
y_t &\sim & f_{0}(y_t ; \lambda_t,  \nu),  \qquad \lambda _t = \exp \, h_t, \\ \nonumber
h_t &=& \mu_h + s_t +x_t , \qquad
s_t = w_t \beta , \qquad
x_{t+1} = \varphi x_t + \eta_t, \nonumber
\end{eqnarray}
where $\eta_t \sim \mathcal{N}\left(0, \sigma^{2}_{\eta}\right)$, for $t=1,\ldots,T$.
The details of the model are discussed in Section \ref{c_sec:model}. The
variable parameters $\nu$, $\mu _h$, $\beta$, $\varphi$ and $\sigma _{\eta} ^2$ are
static while $x_t$ is a latent variable that is modeled as a stationary autoregressive process.
The intra-day seasonal effect $s_t$ is represented by a Poirier spline;
see Appendix \ref{c_sec:spline_appendix}.

Our proposed Bayesian estimation procedure aims to estimate all static variables jointly with the
time-varying signal $h_1,\ldots, h_T$ for the dynamic $\Delta$NB model.
It is based on Gibbs sampling,
data augmentation and auxiliary mixture sampling
methods which are developed by \citet{SFSWagner2006} and \citet{SFSFHeldRue2009}.
At each time point $t$, for $t=1,\ldots,T$,
we introduce a set of latent auxiliary variables
to facilitate the derivation of conditional distributions.
By introducing these auxiliary variables 
we are able to specify the model as a linear state space model
with non-Gaussian observation disturbances. Moreover 
using an auxiliary mixture sampling procedure, we can even obtain  conditionally
an approximating linear Gaussian state space model.
In such a setting, we can exploit highly efficient Kalman filtering and
smoothing procedures for the sampling of many full paths for the dynamic latent
variables.
These ingredients are key for a computational feasible implementation
of our estimation process.
%state vector $x_t$. 

\subsection{Data augmentation: our latent auxiliary variables}

We use the following auxiliary variables for the data augmentation.
We define $N_t$ as the sum of $NB^{+}$ and $NB^{-}$,
the gamma mixing variables $z_{t1}$ and $z_{t2}$.
Moreover conditional on $z_{t1}$ , $z_{t2}$
and the intensity $\lambda_t$, we can interpret $N_t$ as a Poisson process on $[0,1]$ with intensity
$(z_{t1}+z_{t2})\lambda_t$ based on the result in equation
(\ref{c_eq:gammapo}). We can introduce the latent arrival
time of the $N_t$-th jump of the Poisson process $\tau_{t2}$ and
the arrival time between the $N_t$-th and $N_t+1$-th
jump of the process $\tau_{t1}$ for every $t=1, \ldots, T$.
The interarrival time $\tau_{t1}$ can be assumed to come from an
exponential distribution with intensity $(z_{t1}+z_{t2})\lambda_t$
while the $N_t$th arrival time can be treated as the gamma distributed variable
with density function 
$\text{Ga}(N_t, (z_{t1}+z_{t2})\lambda_t)$.
We have
\begin{eqnarray}
\tau_{t1} &=& \frac{\xi_{t1}}{(z_{t1}+z_{t2})\lambda_t}, \quad \xi_{t1} \sim \text{Exp}(1) \\
\tau_{t2} &=& \frac{\xi_{t2}}{(z_{t1}+z_{t2})\lambda_t},  \quad \xi_{t2} \sim \text{Ga}(N_t ,1),
\end{eqnarray}
where we can treat $\xi_{t1}$ and $\xi_{t2}$ as auxiliary variables.
By taking the logarithm of the equations and substituting
the definition of $\log \lambda_t$ from
equation (\ref{eq:logsvspl}), we can rewrite the above equations as 
\begin{eqnarray}
-\log \tau_{t1} &=& \log(z_{t1}+z_{t2})+ \mu_h + s_t +x_t  + \xi^{*}_{t1}, \quad \xi^{*}_{t1} =-\log \xi_{t1} \\
-\log \tau_{t2} &=&  \log(z_{t1}+z_{t2})+ \mu_h + s_t +x_t  + \xi^{*}_{t2}, \quad \xi^{*}_{t2} =-\log \xi_{t2}.
\end{eqnarray}
These equations are linear in the state vector, which would facilitate the use
of Kalman filtering. However, the error terms $\xi^{*}_{t1}$ and $\xi^{*}_{t2}$
are non-normal. We can adopt solutions as in \citet{SFSWagner2006}
and \citet{SFSFHeldRue2009} where the gamma and exponential distributions
are approximated by normal mixture distributions.
In particular, we can specify the approximations as
\begin{equation}
f_{\xi^{*}}(x; N_t)\approx \sum \limits_{i=1}^{C(N_t)} \omega_{i}(N_t) \phi\left(x, m_{i}(N_t),v_{i}(N_t)\right),
\end{equation}
where $C(N_t)$ is the number of mixture components at time $t$, for $t=1,\ldots,T$,
$\omega_{i}(N_t)$ is the weight, and $\phi(x,m,v)$ is the normal density
for variable $x$ with mean $m$ and variance $v$.
These approximations remain to depend on $N_t$ because
the log gamma distribution is not canonical and it has different shapes for different
values of $N_t$.

\subsection{Mixture indicators for obtaining conditional linear model}

Conditional on $N$,$z_{1}$, $z_{2}$,$\tau_{1}$ ,$\tau_{2}$  and
$C=\left\{c_{tj}, t=1, \ldots, T , j=1,\ldots, \min(N_t+1,2) \right\}$ we can write 
the following  state space form
\begin{eqnarray}
\underbrace{\tilde{y}_t }_{\min(N_t+1,2)\times 1}&=&  \underbrace{\left[ 
\begin{tabular}{ccc}
1 &$w_t$ &1 \\
 1 &$w_t$ &1
\end{tabular}
\right]}_{\min(N_t+1,2) \times (K+2)}  \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1}  + \underbrace{\varepsilon_{t}}_{\min(N_t+1,2)\times 1} , \quad \varepsilon_{t} \sim \mathcal{N}(0, \text{H}_t) \\ 
\alpha_{t+1}  &=& \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1} = \underbrace{\left[ 
\begin{tabular}{ccc}
1 &0& 0 \\
0& $I_{K}$ & 0 \\
0 &0& $\varphi$
\end{tabular}
\right]}_{(K+2)\times(K+2)} 
\underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1} +\underbrace{\left[ 
\begin{tabular}{c}
0 \\
0 \\
$\eta_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1}  ,  \eta_{t+1} \sim \mathcal{N}(0,\sigma _{\eta}^{2}) \\ 
\end{eqnarray}
where 
\begin{equation}
\underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta$ \\
$x_{1}$  
\end{tabular}
\right]}_{(K+2)\times 1} \sim \mathcal{N} \left( \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{0}$ \\
$\beta_{0}$ \\
0
\end{tabular}
\right]}_{(K+2)\times 1} ,\underbrace{\left[ 
\begin{tabular}{ccc}
$\sigma^{2}_{\mu}$ & 0& 0 \\
0&$\sigma^{2}_{\beta} I_{K}$ &  0 \\
0& 0& $\sigma _{\eta}^{2}/(1-\varphi^2)$
\end{tabular}
\right]}_{(K+2)\times (K+2)}  \right)  
\end{equation}
$ \text{H}_t = \text{diag}( v^{2}_{c_{t1}}(1), v^{2}_{c_{t,2}}(N_t))$ and 
\begin{eqnarray}
\underbrace{\tilde{y}_t}_{\min(N_t+1,2)\times 1} = \left(
\begin{tabular}{c}
$- \log \tau_{t1}- m_{ c_{t1}}(1)-\log(z_{t1}+z_{t2})$ \\
$- \log \tau_{t2}- m_{ c_{t2}}(N_t)-\log(z_{t1}+z_{t2})$ 
\end{tabular} \right)
\end{eqnarray}

Using the mixture of normal approximation of $\xi^{*}_{t1}$ and $\xi^{*}_{t2}$, allows us to build an efficient Gibbs sampling procedure where we can sample the latent state paths in one block, efficiently using Kalman filtering and smoothing techniques.

%This is crucial as in our application the number of observation is large and updating the state time period by time period would make our estimation slow and inefficient.

\subsection{The sampling of event times $N_t$}

The remaining challenge is the sampling of $N_t$ as all the other
full conditionals are standard. We notice that conditional on $z_{t1}$ , $z_{t2}$
and the intensity $\lambda_t$, the $N_t$'s are independent over time. We have
\begin{equation}
p( N| \gamma, \nu,\mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,z_{1} ,z_{2},y)=
\prod \limits_{t=1}^{T}p( N_t | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t ) ,
\end{equation}
where  the $t$ element vectors $(v_1, \ldots, v_t)$ containing time
dependent variables for all time time periods, are denoted by $v$, the variable without a subscript.
For a given time index $t$, we can draw $N_t$ from a discrete distribution with  
 \begin{eqnarray}
p( N_t  | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t ) &=& \frac{p( N_t, y_t  | \gamma, \lambda_t ,z_{t1} ,z_{t2})}{p(y_t | \gamma, \lambda_t ,z_{t1} ,z_{t2}) }  \nonumber \\
 &=& \frac{p( y_t | N_t, \gamma, \lambda_t ,z_{t1} ,z_{t2})p( N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) }{p(y_t|  \gamma, \lambda_t ,z_{t1} ,z_{t2})} \nonumber \\
 &=& \left[ \gamma \mathbbm{1}_{\left\{y_t=0 \right\}} +(1-\gamma) p\left( y_t |N_t , \lambda_t ,z_{t1} ,z_{t2}\right) \right] \nonumber \\
 &\times &\frac{p( N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) }{p(y_t|  \gamma, \lambda_t ,z_{t1} ,z_{t2})} 
 \label{c_eq:neq}
 \end{eqnarray}
 The denominator in equation (\ref{c_eq:neq}) is a Skellam  distribution  with intensity $\lambda_{t} z_{t1}$ and$\lambda_{t} z_{t2}$. We can calculate  probability 
\begin{equation}
 p\left( y_t |N_t , \lambda_t ,z_{t1} ,z_{t2}\right) 
\end{equation} 
using the results from equation (\ref{c_eq:gammapo})
condition on $\lambda_t$, $z_{t1}$ and $z_{t2}$, $y_t$ is distributed as a marked Poisson process with marks given by 
\begin{equation}
M_i =\begin{cases}
    1, & \text{with} \quad P(M_i = 1)= \frac{z_{t1} }{z_{t1}+z_{t2} } \\
    -1, & \text{with} \quad P(M_i = -1)= \frac{z_{t2} }{z_{t1}+z_{t2} } 
  \end{cases},
\end{equation}
which implies that we can represent $y_t$ as $\sum \limits_{i=0}^{N_t} M_i $.  

% with a tree structure and the binomial distribution. $\sum \limits_{i=1}^{n} M_i $ has a binomial distribution with $n$ trails, $(n+k)/2$ successes and $p=0.5$ success rate. Note that even $k$ can only happen in even number of trails and odd $k$ can only happen in odd number of trails. 
\begin{equation}
p\left( y_t | N_t, \lambda_t, z_{t1} ,z_{t2}\right) =\begin{cases}
    0\quad,\quad  \text{if}  \quad  y_t > N_t \quad \text{or}\quad |y_t| \bmod 2\neq |N_t| \bmod 2 &    \\
    \displaystyle \binom{N_t}{\frac{N_t+y_t}{2}} \left(\frac{z_{t1} }{z_{t1}+z_{t2}} \right)^{\frac{N_t+y_t}{2}}\left(\frac{z_{t2} }{z_{t1}+z_{t2}} \right)^{\frac{N_t-y_t}{2}} , & \text{otherwise}
  \end{cases}
\end{equation}

 Conditional  on $z_{t1}$ , $z_{t2}$
and  $\lambda_t$, $N_t$ is a realization of a Poisson process on $[0,1]$ with intensity $(z_{t1}+z_{t2})\lambda_t$, hence the probability $p(  N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) $  is a Poisson random variable with intensity equal to $\lambda_t ( z_{t1}+z_{t2})$.
We can draw $N_t $ parallel over $t= 1, \ldots, T$ by drawing a uniform random variable $u_t \sim U[0,1]$ and 
\begin{equation}
N_t = \min  \left\{ n : u_t \leq \sum \limits_{i=0}^{n} p( i  | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t )  \right\}
\end{equation}

\subsection{Markov chain Monte Carlo algorithm}

The complete MCMC algorithm is outlined below.
Various details of the MCMC steps are presented in Appendix \ref{c_sec:dnb_appendix}.
In an algorithmic style, the MCMC steps are given as follows.
\begin{enumerate}
 \item Initialize $\mu_{h}$, $\varphi$, $\sigma^{2}_{\eta}$, $\gamma$, $\nu$, $C$ , $\tau$, $N$, $z_1$, $z_2 $, $s$ and $x$ 
\item \label{DNBstep_2}  Generate $\varphi$, $\sigma^{2}_{\eta}$, $\mu_{h},s$  and $x$  from $p(\varphi, \sigma^{2}_{\eta} ,\mu_{h},s ,x| \gamma, \nu, C, \tau, N,z_1 , z_2, s ,y)$
\begin{enumerate}
\item Draw $\varphi, \sigma^{2}_{\eta}$ from $p(\varphi,\sigma^{2}_{\eta}  | \gamma, \nu, C , \tau, N,z_1 , z_2,s ,y)$
\item Draw $\mu_{h},s$ and $x$ from $p(\mu_{h},s ,x| \varphi,\sigma^{2}_{\eta} ,\gamma, \nu, C, \tau, N,z_1 , z_2,s ,y)$
\end{enumerate}
\item Generate $\gamma$ from $p(\gamma | \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},x ,C, \tau, N,z_1 , z_2,s ,y)$
%\item Generate $s $ from $p( s | \gamma, \mu_{h} ,\nu,\phi,\sigma^{2}_{\eta},x ,R , \tau, N, z_1 , z_2, y)$
\item Generate $ C , \tau, N, z_1 , z_2, \nu $ from $p(C , \tau, N,z_1 , z_2, \nu |\gamma, \mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s ,y)$
\begin{enumerate}
\item Draw $\nu$ from $p(\nu | \gamma, \mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s,y)$
\item  Draw $z_1 , z_2 $ from $p(z_1 , z_2  |\nu,\gamma,\mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s ,y )$
\item Draw $N$ from $p(N |z_1 , z_2, \nu,\gamma,\mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s ,y )$
\item Draw $\tau$ from $p( \tau |N, z_1 , z_2,\nu,\gamma,\mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s ,y )$
\item Draw $C$ from $p(C | \tau, N,z_1 , z_2,\nu,\gamma, \mu_{h} ,\varphi,\sigma^{2}_{\eta},x,s ,y )$
\end{enumerate}
\item Go to \ref{DNBstep_2}
\end{enumerate} 

 


To validate our estimation procedure for the dynamic Skellam and $\Delta$NB models
we simulate $20,000$ observation and apply our MCMC procedure with $100,000$ replications,
in a single experiment.
Our true parameters are chosen as $\mu=-1.7$, $\varphi=0.97$, $\sigma _{\eta}=0.02$, $\gamma=0.001$
and $\nu=15$ which are close to those estimated from real data in our empirical study
of Section \ref{c_sec:empirical}.
In Table \ref{c_table:sim_res} we summarize the results of our estimates and their
corresponding highest posterior density (HPD) regions.
The results indicate that in our stylized setting,
the algorithm can estimate the parameters accurately since the
true parameters are within the HPD regions based on the estimates.
The posterior distributions of the parameters for the $\Delta$NB model are
presented in Figure \ref{c_pic:dnb_res_pic};
those for the dynamic Skellam model are presented in Appendix \ref{secapp:mcmcdnb}.
The most atypical posterior distributions are displayed for the
autoregressive coefficient $\varphi$ and for the state variance $\sigma _{\eta} ^2$.
Hence we may conclude that these parameters are the most challenging to estimate.  



\begin{table}[!htp]
\begin{center}
\caption{Estimation results from a dynamic Skellam and $\Delta$NB model based on
$20,000$ observations and $100,000$ iterations from which $20,000$ used
as a burn in sample.The 95 \% HPD regions are in brackets.}
%The true parameters are  $\mu=-1.7$, $\varphi=0.97$ , $\sigma=0.02$, $\gamma=0.001$
%and $\nu=15$}
\input{Tables/ResultTable0000.tex}
\label{c_table:sim_res}
\end{center}
\end{table}


\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.65]{Pictures/0000_DNB_DNBdens_all.pdf}
       \caption{Posterior distributions of the parameters from a dynamic $\Delta$ NB model based on 20000 observations and 100000 iterations from which 20000 used as a burn in sample. Each picture shows the histogram of the posterior draws the kernel density estimate of the posterior distribution, the HPD region and the posterior mean. The true parameters are   $\mu=-1.7$, $\varphi=0.97$ , $\sigma _{\eta}=0.02$, $\gamma=0.001$ and $\nu=15$. }
 \label{c_pic:dnb_res_pic}
 \end{figure}  


\section{Empirical study}
\label{c_sec:empirical}

In this section we present and discuss the empirical findings from our analyses
concerning tick by tick price changes for six different stocks traded at the NYSE,
for two different periods.
We consider two model classes and two models for each class.
The first set consists of the ordered probit models with
normal and Student's $t$ stochastic volatility.
The second set includes the dynamic Skellam and dynamic $\Delta$NB models.
The analyses include in-sample and out-of-sample marginal likelihood comparison
of the models.
Our aims of the empirical study is twofold.
First, the usefulness of the $\Delta$NB model on a challenging dataset is investigated.
In particular, we validate our estimation procedure and reveal possible
shortcomings in the estimation of the parameters in the $\Delta$NB model.
Second, we intend to find out what the differences are when the considered models
are based on heavy-tailed distributions (ordered $t$ and $\Delta$NB models)
or not (ordered normal and dynamic Skellam models).
Also, we compare the different model classes: ordered model
versus integer distribution model.

\subsection{Data}

We have access to the Thomson Reuters Sirca dataset that
contains all trades and quotes with millisecond time
stamps for all stocks listed at NYSE.
We have collected the data for Alcoa (AA), Coca-Cola (KO),
International Business Machines (IBM),
J.P. Morgan (JPM), Ford (F) and Xerox (XRX).
These stocks differ in liquidity and in price magnitude.
In our study we concentrate on two weeks of price changes:
the first week of October 2008 and the last week of April 2010.
These weeks exhibit different market sentiments and
volatility characteristics. The month of October 2008 is in the
middle of the 2008 financial crises with record high 
volatilities and some markets experienced their
worst weeks in October 2008 since 1929.
The month of April 2010 is a much calmer month with low volatilities. 

The cleaning process of the data consists of a number of filtering steps that are
similar to the procedures described in \citet{BoudtCornelissenPayseur},
\citet{BNHansenLundeShephard2008} and  \citet{BrownleesGallo2006}.
First, we remove the quotes-only entries which is a large portion of the data.
By excluding the quotes we lose around $70-90\%$ of the data.
In the next step, we delete the trades with missing or zero prices or volumes.
We also restrict our analysis to the trading period.
The fourth step is to aggregate the trades which have
the same time stamp. We take the trades with the last
sequence number when there are multiple trades at the same millisecond.
We regard the last price as the price that
we can observe with a millisecond resolution.
Finally, we treat outliers by following the rules as suggested by
\citet{BNHansenLundeShephard2008}.
We delete trades with prices smaller then the bid-price minus the bid-ask spread
and higher than the ask-price plus the bid-ask spread. 
Tables \ref{c_table:desc2008} and \ref{c_table:desc2010}
present the descriptive statistics for our resulting data from
the 3rd to 10th October 2008 and from the 23rd to 30th April 2010, respectively.
A more detailed account of the cleaning process can be found in
Tables \ref{c_table:data2008} and \ref{c_table:data2010} in Appendix \ref{c_sec:dataclean}.
We treat the periods from the 3rd to 9th October 2008 and from the 23rd to 29th April 2010 as
the in-sample periods. The two out-of-sample periods are 10 October 2008 and 30 April 2010.

\begin{table}[!htp]
\begin{center}
\caption{Descriptive statistics of the data from 3rd to 10th October 2008. Column In displays the statistics on the in-sample period from 3rd to 9th October 2008, while the column Out shows the descriptives for the out-of-sample period 10th October. We show the number of observations (Num.obs), average price (Avg. price), mean price change (Mean), standard deviation of price changes (Std), minimum and max integer price changes (Min,Max) and the percentage of zeros in the sample ($\%$ Zeros).} 
\begin{small}
\input{Tables/desc2008_table.tex}
\end{small}
\label{c_table:desc2008}
\end{center}
\end{table}

\begin{table}[!htp]
\begin{center}
\caption{Descriptive statistics of the data  from 23rd to 30th April 2010. Column In displays the statistics on the in-sample period from 23rd to 29th October 2008, while the column Out shows the descriptives for the out-of-sample period 30th October. We show the number of observations (Num.obs), average price (Avg. price), mean price change (Mean), standard deviation of price change (Std), minimum and max integer price change (Min,Max) and the percentage of zeros in the sample ($\%$ Zeros).} 
\begin{small}
\input{Tables/desc2010_table.tex}
\end{small}
\label{c_table:desc2010}
\end{center}
\end{table}

\subsection{Estimation results}

We start our analyses with the dynamic Skellam and $\Delta$NB models
for all considered stocks  in the periods from
3rd to 9th October 2008 and from 23rd to 29th April 2010.
In this study, after some initial experimentation,
we use the following prior distributions
\[
\mu_h \sim \mathcal{N}(0,1), \qquad \beta_i \sim \mathcal{N}(0,1), \qquad
\frac{\varphi+1}2 \sim \mathcal{B}(20,1.5),
\]
\[
\sigma _{\eta}^2\sim \mathcal{IG}(2.5,0.025), \qquad
\gamma \sim \mathcal{B}(1.7,10), \qquad
\nu \sim \mathcal{DU}(2,128),
\]
for $i=1,\ldots,K$,
where $\mathcal{N}$ is the normal, $\mathcal{B}$ is the beta,
$\mathcal{IG}$ is the inverse gamma, and $\mathcal{DU}$ is the discrete uniform
distribution.
In the MCMC procedure, 
we draw $100,000$ samples from the Markov chain and disregard the first $20,000$
draws as burn-in samples.
The results of parameter estimation for the 2008 and 2010 data periods
are reported in Tables \ref{c_table:emp_res_2008_1},  \ref{c_table:emp_res_2008_2}, \ref{c_table:emp_res_2008_3} and Tables
\ref{c_table:emp_res_2010_1}, \ref{c_table:emp_res_2010_2}, \ref{c_table:emp_res_2010_3} respectively.


\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for Alcoa (AA) and Ford (F) 
during the period from 3rd to 9th October 2008.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2008_1.tex}
\label{c_table:emp_res_2008_1}
\end{center}
\end{table} 

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for International Business Machines (IBM) and JP Morgan (JPM)
during the period from 3rd to 9th October 2008.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2008_2.tex}
\label{c_table:emp_res_2008_2}
\end{center}
\end{table} 

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for Coca-Cola (KO) and Xerox (XRX)
during the period from 3rd to 9th October 2008.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2008_3.tex}
\label{c_table:emp_res_2008_3}
\end{center}
\end{table} 

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for Alcoa (AA) and Ford (F) 
during the period from 23rd to 29th April 2010.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2010_1.tex}
\label{c_table:emp_res_2010_1}
\end{center}
\end{table}

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for International Business Machines (IBM) and JP Morgan (JPM)
during the period from 23rd to 29th April 2010.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2010_2.tex}
\label{c_table:emp_res_2010_2}
\end{center}
\end{table}

\begin{table}[htbp]
\begin{center}
\caption{\footnotesize Estimation results from a dynamic Skellam and  $\Delta$ NB model for Coca-Cola (KO) and Xerox (XRX)
during the period from 23rd to 29th April 2010.
The posterior mean estimates are based on 100,000 iterations
(20,000 used as burn-in).
The 95 \% HPD regions are in brackets.
MaxIneff and minESS are maximum inefficiency among parameters
and minimum effective sample size, respectively.} 
\input{Tables/ResultTable2010_3.tex}
\label{c_table:emp_res_2010_3}
\end{center}
\end{table}

The unconditional mean volatility differ across stocks and time periods.
The unconditional mean of the latent state is higher for stocks with
higher price and it is higher in the more volatile periods in 2008.
These results are consistent with intuition but we should not take strong
conclusions from these findings.
For example, we cannot compare the means between models as they have
somewhat different meanings in different model specifications.
The estimated AR(1) coefficients for the different series range from $0.88$ to $0.99$.
This finding suggests persistent dynamic volatility behaviour within a trading day, even
after accounting for the intra-day seasonal pattern in volatility.
However, by comparing the two different periods, we find that the
transient volatility is less persistent in
the more volatile crises period.
%The volatility persistence parameter $\varphi$
%is estimated as a higher value during the 2008 financial crisis period.
We only included the zero inflation specification for the $\Delta$NB and dynamic Skellam
distributions when additional flexibility appears to be needed in the observation density.
This flexibility has been required for higher price stocks and during
the more volatile periods.
In case of the April 2010 period we used the zero inflation only for IBM,
while in the October 2008 period we included the zero inflation for all
stocks expect for the two lowest price stocks F and XRX.
The estimates for the zero inflation parameter $\gamma$ ranges from 0.1 to 0.3.
The degrees of freedom parameter $\nu$ for the $\Delta$NB distribution is estimated as
a higher value during the more quiet 2010 period which suggests that the distribution
of the tick by tick price change is closer to a thin tailed distribution
during such periods.
In addition, we have found that the estimated degrees of freedom parameter
is a lower value for stocks with a higher average price.

From a more technical perspective, our study has revealed that 
the parameters of our $\Delta$NB modeling framework mix relatively slowly.
This may indicate that our procedure can be rather inefficient.
However, it turns out that the troublesome parameters are in all
cases the persistence parameter of the volatility process, $\varphi$, and
the volatility of volatility, $\sigma _{\eta}$.
It is well established and documented that these coefficients are not easy to
estimate as they have not a direct impact on the observations as such; see
the discussions in \citet{KimShephardChib1998} and \citet{JohannesStroud2014}).
Furthermore, our empirical study is faced with some challenging numerical problems.
First, we should emphasize that for some stock we analyze almost $100,000$ observations, and the shortest time series has around  $30,000$ observations.
Such a long time series will typically lead us to a slow
mixing process in a Bayesian MCMC estimation process because the full conditional distributions
are highly informative. Hence the role of some parameters, specifically those in
the state equation, in the estimation process is rather weak.
However, we do not conclude that we cannot estimate
such parameters accurately. Our simulated experiment in the
previous section has shown that our algorithm is successful.
It just requires more numerical efforts to obtain accurate results.
Second, we have anticipated that parameter estimation for the dynamic
Skellam and $\Delta$NB models is less numerically efficient and overall more challenging
when compared to the estimation for ordered normal and ordered $t$ models.
Parameter estimation for the discrete distribution models requires more auxiliary
variables and the analysis is based on additional conditional statements.

%\begin{table}[!htp]
%\begin{center}
%\caption{Estimation results from a dynamic Skellam and  $\Delta$ NB model
%during the period from 3rd to 9th October 2008.
%The posterior mean estimates are based on 100,000 iterations
%(20,000 used as burn-in).
%The 95 \% HPD regions are in brackets.
%MaxIneff and minESS are maximum inefficiency among parameters
%and minimum effective sample size, respectively.} 
%\input{Tables/ResultTable2008.tex}
%\label{c_table:emp_res_2008}
%\end{center}
%\end{table} 




%\begin{table}[!htp]
%\begin{center}
%\caption{Estimation results from a dynamic Skellam and  $\Delta$ NB model
%during the period from 23rd to 29th April 2010.
%The posterior mean estimates are based on 100,000 iterations
%(20,000 used as burn-in).
%The 95 \% HPD regions are in brackets.
%MaxIneff and minESS are maximum inefficiency among parameters
%and minimum effective sample size, respectively.} 
%\input{Tables/ResultTable2010.tex}
%\label{c_table:emp_res_2010}
%\end{center}
%\end{table}

\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.7]{Pictures/KO_season2.pdf}
       \caption{Decomposition of log volatility in the dynamic Skellam model for KO from 23rd to 29th April 2010.}
 \label{c_pic:KO_season}
 \end{figure}
 
On the basis of the output of our MCMC estimation procedure,
we obtain the estimates for the latent volatility variable $h_t$ but we
can also decompose these estimates into
the corresponding components of $h_t$, these are $\mu_{h}$, $s_t$ and $x_t$; see equation
\eqref{eq:logsvspl}.
Figure \ref{c_pic:KO_season} presents the intra-day, tick by tick Coca Cola price changes
and its estimated components $s_t$ and $x_t$ 
for the logarithm of volatility $h_t$ in the Skellam model,
from 23rd to 29th April 2010.
We notice that apart from the pronounced intra-day seasonality in volatility, many
volatility changes occur within a trading day. Some of these volatility
changes may have been sparked by news announcements while others may have occurred as
the result of the trading process.
 

\subsection{In-sample comparison}

It is widely established in Bayesian studies that the computation of sequential  Bayes factors ($BF$) is infeasible
in this framework as it requires sequential parameter estimation.
The sequential estimation of the parameters in our model is computationally
prohibitive given the very high time dimensions.
To provide some comparative assessments of the four models that we have considered in
our study, we follow \citet{JohannesStroud2014} and calculate Bayesian Information
Criteria ($BIC$) for model $\mathcal{M}$ as
\begin{equation}
BIC_{T}(\mathcal{M})=-2\sum \limits_{t=1}^{T} \log p(y_t|\hat{\theta},\mathcal{M})+d_i \log T 
\end{equation} 
where $ p(y_t|\theta,\mathcal{M})$ can be calculated by means of a particle filter and 
$\hat{\theta}$ is the posterior mean of the parameters.
The implementation of the particle filter for all considered models is rather
straightforward given the provided details of the models in Section \ref{c_sec:model}.
The BIC gives an asymptotic approximation to the Bayes
factor by
\[
BIC_{T}(\mathcal{M}_i)-BIC_{T}(\mathcal{M}_j)\approx -2 \log BF_{i,j}.
\]
We will use this approximation for our sequential model comparisons. 

Figures \ref{c_pic:2008_in} and Figure \ref{c_pic:2010_in} present the in-sample
Bayes factors for the periods from 23rd to 29th October 2008 and from
3rd to 9th April 2010, respectively.
These graphs are rather insightful as they indicate that for all stocks, no evidence in
favour of the $\Delta$NB model can be detected for the 2008 period.
In the 2010 period, only the IBM stock favours the $\Delta$NB distribution.
In 2008 on the lower price stocks  AA, F and XRX,  the ordered $t$ model seems to
provide the best fit.
In 2010 the ordered normal model performs the best on the lower priced stocks,
while the high priced stocks are treated more successfully with fat tailed distributions
such as the ordered $t$ or the $\Delta$NB distribution models.
% This result is consistent with our prior conjecture that in the crisis period there are more jumps. 
Furthermore, we may conclude from 
the sequential Bayes factor results that the ordered $t$ and $\Delta$NB model tends
to be favoured in case sudden big jumps in volatility have occurred.
Such large to extreme realizations of price changes, possibly leading to
a prolonged period of high volatility, suggest the need of the $\Delta$NB model.
These findings are consistent with the intuition that for time varying volatility
models, the identification of parameters determining the tail behaviour
requires extreme or excessive observations in combination with low volatility.

%The distribution of integer price changes shows different
%characteristics based on the price of the stock and volatility
%during the observed period, which can be due to the fact that,
%these are not properly scaled returns just price differences. 



\subsection{Out-of-sample comparisons}

The performances of the dynamic Skellam and $\Delta$NB models can also be compared
in terms of predictive likelihoods.
The one-step-ahead predictive likelihood for model $\mathcal{M}$ is 
\begin{eqnarray}
& & p(y_{t+1}|y_{1:t},\mathcal{M}) \ = \nonumber  \\
& & \int \int  p(y_{t+1}|y_{1:t},x_{t+1}, \theta,\mathcal{M})p( x_{t+1}, \theta|y_{1:t},\mathcal{M}) \dd x_{t+1} \dd \theta  \ = \nonumber \\
& & \int \int  p(y_{t+1}|y_{1:t},x_{t+1}, \theta,\mathcal{M})p( x_{t+1}| \theta , y_{1:t},\mathcal{M})p( \theta | y_{1:t},\mathcal{M}) \dd x_{t+1} \dd \theta.
\end{eqnarray}
Generally, the $h$-step-ahead predictive likelihood can be decomposed to the sum
of one-step-ahead predictive likelihoods via
\begin{eqnarray}
p(y_{t+1:t+h}|y_{1:t},\mathcal{M})
&=&\prod \limits_{i=1}^{h} p(y_{t+i}|y_{1:t+i-1},\mathcal{M})= \prod \limits_{i=1}^{h} \int \int  p(y_{t+i}|y_{1:t+i-1},x_{t+i}, \theta,\mathcal{M}).\nonumber  \\
&\times&  p( x_{t+i}| \theta , y_{1:t+i-1},\mathcal{M})p( \theta | y_{1:t+i-1},\mathcal{M}) \dd x_{t+i}. \dd \theta 
\end{eqnarray}
These results suggest that we require the computation of $p( \theta | y_{1:t+i-1},m)$,
for $i=1,2,\ldots$,
that is the posterior of the parameters using sequentially increasing data samples.
It requires the MCMC procedure to be repeated as many times as we have number of
out-of-sample observations.
In our application, for each stock and each model, it implies several thousands
of MCMC replications for a predictive analysis of a single out-of-sample day.
This exercise is computationally not practical or even infeasible.
However, we may be able to rely on the approximation
\begin{eqnarray}
p(y_{t+1:t+h}|y_{1:t},\mathcal{M})&\approx&\prod \limits_{i=1}^{h} \int \int  p(y_{t+i}|y_{1:t+i-1},x_{t+i}, \theta,\mathcal{M})\nonumber \\
&\times &p( x_{t+i}| \theta , y_{1:t+i-1},\mathcal{M})p( \theta | y_{1:t},\mathcal{M}) \dd x_{t+i} \dd \theta.
\end{eqnarray}
This approximation is based on the notion that,
after observing a considerable amount of data,
that is for $t$ sufficiently large,
the posterior distribution of the static parameters should not change much and
hence $p( \theta | y_{1:t+i-1},\mathcal{M}) \approx p( \theta | y_{1:t},\mathcal{M})$.

Based on this approximation, we carry out the following exercise.
From our MCMC output we obtain a sample of posterior distributions
based on the in-sample observations.
For each parameter draws from the posterior distribution we
estimate the likelihood using the particle filter for the out-of-sample period.

Figures \ref{c_pic:2008_pred} and Figure \ref{c_pic:2010_pred} present
the out-of-sample sequential predictive Bayes factors for the
10th October 2008 and 30th April 2010, respectively. %Based on the  results we can say that in the more volatile period the $\Delta$NB model is doing better in term of Bayes factor. On 10th October AA, KO,XRX show evidence in favour the $\Delta$NB model in the out-of-sample comparison, while on 30th October 2008 the dynamic Skellam model fits the data better out-of sample, except for IBM. 
On the 10th October 2008, the dynamic Skellam model is preferred for
IBM and JPM, the ordered normal model is preferred for AA, F and KO while the
$\Delta$NB model is the preferred model for XRX.
The dynamic Skellam model for IBM and JPM is consistently winning for
both the in-sample and out-of-sample periods.
On 30th April 2010 the ordered $t$ model performs the best for AA,
F, JPM ,KO and XRX, while the $\Delta$NB model is the best for IBM.
The different models appear only to be consistent in terms of the
in-sample and out-of-sample performances for the high price stocks.   


\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.7]{Pictures/2008_InLL.pdf}
       \caption{ Sequential Bayes factors approximation based on BIC  on data from 3rd to 9th October 2008.}
 \label{c_pic:2008_in}
 \end{figure} 
\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.7]{Pictures/2008_OutLL.pdf}
       \caption{ Sequential predictive Bayes factors on 10th October 2008.}
 \label{c_pic:2008_pred}
 \end{figure}  
\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.7]{Pictures/2010_InLL.pdf}
       \caption{ Sequential Bayes factors approximation based on BIC  on data from  23rd to 29th  April 2010.}
 \label{c_pic:2010_in}
 \end{figure} 
\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.7]{Pictures/2010_OutLL.pdf}
       \caption{ Sequential predictive Bayes factors on 30th April 2010.}
 \label{c_pic:2010_pred}
 \end{figure}  



\section{Conclusion}\label{c_sec:concl}

We have reviewed and introduced dynamic models for high-frequency integer price changes.
In particular, we have introduced
the dynamic negative binomial difference model, referred to as the $\Delta$NB model.
We have developed a Markov chain Monte Carlo procedure (based on Gibbs sampling)
for the Bayesian estimation of parameters in the dynamic Skellam and $\Delta$NB models.
Furthermore, we have demonstrated our estimation procedures for simulated data and
for real data consisting of tick by tick prices from NYSE stocks.
We have compared the in-sample and out-of-sample
performances of the different models.
 
%  \FloatBarrier  
%\addcontentsline{toc}{section}{References}
\bibliographystyle{chicago}
\bibliography{NB_bib}

\newpage
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{equation}{0}
\appendix

%\section{Data}
%\label{c_sec:data_appendix}



%\section{Numerical issues with the Skellam distribution}
%In general it is good to use the scaled version of the modified Bessel function of the first kind
%\begin{equation}
%\exp(-z) I_{n}(z).
%\end{equation}
%For the special case of the Skellam
%\begin{equation}
%\exp(-2 \lambda )  I_{|k|} ( 2 \lambda) 
%\end{equation}
%
%for small and large $2 \lambda$ this still can be unstable, but we can use the following approximations
%for small $2 \lambda$
%\begin{equation}
%\exp(-2 \lambda )  I_{|k|} ( 2 \lambda) \approx \exp(-2 \lambda) \frac{1}{\Gamma(|k|+1)}\left( \frac{2 \lambda}{2}\right)^{|k|} \approx 1 \times \frac{ \lambda^{|k|}}{\Gamma(|k|+1)}
%\end{equation}
%While for large $2 \lambda$ we can use  
%\begin{equation}
%\exp(-2 \lambda )  I_{|k|} ( 2 \lambda) \approx  \exp(-2 \lambda)\frac{\exp(2 \lambda)}{\sqrt{2\pi 2 \lambda }} =\frac{1}{2\sqrt{\pi \lambda }}
%\end{equation}
%\subsection{Dynamic Skellam  model representation}
%The simplest from of the dynamic Skellam model is given with the following state space representation
%\begin{equation}
%r_t  | \theta_t \sim \textrm{Skellam}( \lambda_t ) \quad \lambda_{t}=\exp(\theta_t)
%\end{equation}
%\begin{equation}
%\theta_t |\theta_{t-1} \sim \textrm{N}( c + \phi \theta_{t-1} , \sigma^2_{\eta}),
%\end{equation}
%%+ \eta_t \quad \eta_t 
%with $\theta_1 \sim N(c/(1-\phi), \sigma^2_{\eta}/(1-\phi^2))$.
%This means
%\begin{equation}
%\textrm{E}(r_t | \theta_t)= 0
%\end{equation}
%and
%\begin{equation}
%\textrm{Var}(r_t | \theta_t)= 2 \exp(\theta_t )
%\end{equation}





\section{Negative Binomial distribution}
\label{c_sec:nb}
Different parametrization of the NB distribution
\begin{equation}
f(k;\nu, p)= \frac{\Gamma(\nu+k)}{\Gamma(\nu)\Gamma(k+1)} p^{k} (1-p)^{\nu}
\end{equation}
Using 
\begin{equation}
 \quad  \quad \lambda=\nu\frac{p}{1-p} \Rightarrow p=\frac{\lambda}{\lambda+\nu}
\end{equation}

\begin{equation}
f(k;\lambda, \nu)= \frac{\Gamma(\nu+k)}{\Gamma(\nu)\Gamma(k+1)} \left(\frac{\lambda}{\nu+\lambda} \right)^{k} \left(\frac{\nu}{\nu+\lambda} \right)^{\nu} 
\end{equation}
Mean
\begin{equation}
\mu=\lambda
\end{equation}
Variance
\begin{equation}
\sigma ^2 =\lambda \left(1+\frac{\lambda}{\nu}\right)
\end{equation}
Dispersion index
\begin{equation}
 \frac{\sigma^2}{\mu}\left(1+\frac{\lambda}{\nu}\right) 
\end{equation}
The NB distribution is over dispersed and which means that  there are more intervals with low counts and more intervals with high counts, compared to a Poisson distribution. As we increase $\nu$ we get back to the Poisson case. 


The Poisson distribution can be obtained from the NB distribution as follows
\begin{eqnarray}
\lim_{\nu \rightarrow \infty} f(k;\lambda, \nu)&=&\frac{\lambda^k }{k!} \lim_{\nu \rightarrow \infty}  \frac{\Gamma(\nu+k)}{\Gamma(\nu)(\nu+\lambda)^k}\frac{1}{\left( 1+\frac{\lambda}{\nu} \right)^{\nu} }=\frac{\lambda^k }{k!} \lim_{\nu \rightarrow \infty}  \frac{(\nu+k-1)\ldots \nu}{(\nu+\lambda)^k}\frac{1}{\left( 1+\frac{\lambda}{\nu} \right)^{\nu} } \nonumber \\ &=& \frac{\lambda^k }{k!} \cdot 1 \cdot  \frac{1}{e^\lambda} = \text{Poi}(\lambda)
\end{eqnarray}
%\subsection{NB distribution as a gamma Poisson mixture}
The NB distribution  $Y \sim NB(\lambda, \nu)$ can be written as a Poisson-Gamma mixture or Poisson distribution with Gamma heterogeneity where the Gamma heterogeneity has mean 1.
\begin{equation}
Y \sim \text{Poi}(\lambda U) \quad \text{where} \quad U \sim \text{Ga}(\nu,\nu),
\end{equation}
where we use the $\text{Ga}(\alpha,\beta)$ is given by
\begin{equation}
f(x; \alpha, \beta)= \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x} }{\Gamma(\alpha)}
\end{equation}

 \begin{eqnarray}
f(k;\lambda,\nu)&=& \int \limits_{0}^{\infty} f_{\text{Poisson}}(k; \lambda u ) f_{\text{Gamma}}(u; \nu, \nu) \dd u \nonumber \\
&=&  \int \limits_{0}^{\infty} \frac{(\lambda u)^k e^{-\lambda u}}{k!} \frac{\nu^\nu u^{\nu} e^{-\nu u}}{ \Gamma(\nu)} \dd u \nonumber \\
&=&  \frac{\lambda^k \nu^{\nu}}{k! \Gamma(\nu)} \int \limits_{0}^{\infty}
 e^{-(\lambda+\nu)u} u^{k+\nu-1}
\dd u \nonumber 
\end{eqnarray}
Substituting $(\lambda+\nu)u=s$ we get 
 \begin{eqnarray}
&=& \frac{\lambda^k \nu^{\nu}}{k! \Gamma(\nu)} \int \limits_{0}^{\infty}
 e^{-s} \frac{s^{k+\nu-1}}{(\lambda+\nu)^{k+\nu-1}} \frac{1}{(\lambda+\nu)}
\dd s \nonumber \\
&=&  \frac{\lambda^k \nu^{\nu}}{k! \Gamma(\nu)}  \frac{1}{(\lambda+\nu)^{k+\nu}}\int \limits_{0}^{\infty}
 e^{-s} s^{k+\nu-1}
\dd s \nonumber \\ 
&=&    \frac{\lambda^k \nu^{\nu}}{k! \Gamma(\nu)}  \frac{\Gamma(k+\nu)}{(\lambda+\nu)^{k+\nu}}
\nonumber \\  
&=&  \frac{\Gamma(\nu+k)}{\Gamma(\nu)\Gamma(k+1)} \left(\frac{\lambda}{\nu+\lambda} \right)^{k} \left(\frac{\nu}{\nu+\lambda} \right)^{\nu}   \nonumber \\ 
\end{eqnarray} 
\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.8]{Pictures/Skellam.pdf}
       \caption{ The picture shows the Skellam distribution with different parameters }
 \label{c_pic:skellam_pic}
 \end{figure} 
 
\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.8]{Pictures/DNB.pdf}
       \caption{ The picture shows the $\Delta$NB distribution with different parameters }
 \label{c_pic:dnb_pic}
 \end{figure}  

\section{Daily volatility patterns}
\label{c_sec:spline_appendix}
%There is a strong daily volatility pattern in high frequency returns. The volatility shows a U shaped pattern during the day  with high volatility close to the opening and closing of the market and low volatility during lunch time. We can model this intra day seasonality using splines. We can decompose log volatility signal $\theta$ to
%\begin{equation}
%\text{Total log volatility} : \theta_t=\mu_{h}+x_t +s_t \\
%\end{equation}
%where $s_t$ is intra day seasonality. See for example \citet{Bos2008}, \citet{JohannesStroud2014} and\citet{WeinbergBrownStroud2007}

We want to approximate the function $f: \mathbb{R} \rightarrow \mathbb{R}$ with a continuous function which is built up from piecewise  polynomials of degree at most three. 
Let the set $\Delta = \left\{ k_0, \ldots, k_K \right\} $ denote the set of of knots $k_j$ $j=0, \ldots , K$. $\Delta$ is some times called a mesh on $\left[k_0 , k_K \right]$. Let $y = \left\{ y_0, \ldots, y_K \right\} $ where $y_j = f(x_j)$. We denote a cubic spline on $\Delta$ interpolating to y as $S_{\Delta}(x)$. $S_{\Delta}(x)$ has to satisfy 
\begin{enumerate}
\item $S_{\Delta}(x) \in C^{2}\left[ k_0 , k_K \right] $
\item $S_{\Delta}(x)$ coincides with a polynomial of degree at most three on the intervals $\left[ k_{j-1} , k_j \right]$ for $j=0, \ldots ,K$. 
\item $S_{\Delta}(x)= y_j$  for $j=0, \ldots ,K$. 
\end{enumerate}
Using the 2 we know that the $S^{''}_{\Delta}(x)$ is a linear function on  $\left[ k_{j-1} , k_j \right]$ which means that we can write $S^{''}_{\Delta}(x)$ as 
\begin{equation}
S^{''}_{\Delta}(x) = \left[ \frac{k_j - x }{h_j} \right] M_{j-1} + \left[ \frac{ x - k_{j-1} }{h_j} \right] M_{j}  \quad \text{for} \quad x \in \left[ k_{j-1} , k_j \right]
\end{equation}
where $M_j = S^{''}_{\Delta}(k_j)$ and $h_j = k_j - k_{j-1}$. Integrating $S^{''}_{\Delta}(x) $ and solving the integrating for the two integrating constants (using $S_{\Delta}(x)= y_j$) \citet{Poirier1973} shows that  we get 
\begin{equation}
S^{'}_{\Delta}(x) = \left[ \frac{h_j}{6} - \frac{(k_j - x)^2 }{2 h_j} \right] M_{j-1} + \left[ \frac{ (x - k_{j-1})^2 }{2 h_j} - \frac{h_j}{6} \right] M_{j} + \frac{y_j - y_{j-1}}{h_j} \quad \text{for} \quad x \in \left[ k_{j-1} , k_j \right]
\end{equation}
\begin{eqnarray}
S_{\Delta}(x) &=&  \frac{k_j - x}{ 6 h_j }\left[ (k_j - x)^2 - h^{2}_j \right] M_{j-1} + \frac{x - k_{j-1}}{ 6 h_j }\left[ (x -k_{j-1} )^2 - h^{2}_j \right] M_{j} \nonumber \\
 &+& \left[  \frac{k_j - x}{ h_j }\right] y_{j-1} + \left[  \frac{ x - k_{j-1}}{ h_j }\right] y_{j} \quad \text{for} \quad x \in \left[ k_{j-1} , k_j \right]
 \label{c_eq:spline}
\end{eqnarray}
In the above expression only $M_{j}$ for  $j= 0 , \ldots , K$ are unknown. We can use the 
continuity restrictions which enforce continuity at the knots by requiring that the derivatives are equal at the knots $k_j$ for $j=1, \ldots , K-1$
\begin{equation}
S^{'}_{\Delta}(k^{-}_j)= h_j M_{j-1} / 6 +  h_j M_j / 3  + (y_j - y_{j-1})/h_j
\end{equation}
\begin{equation}
S^{'}_{\Delta}(k^{+}_j)=- h_{j+1} M_{j} / 3 - h_{j+1} M_{j+1} / 6  + (y_{j+1} - y_{j})/h_{j+1}
\end{equation}
which yields $K-1$ conditions
\begin{equation}
(1-\lambda_j ) M_{j-1} + 2 M_{j} + \lambda_j M_{j+1}= \frac{6 y_{j-1}}{h_j (h_j+h_{j+1})} - \frac{6 y_{j}}{h_j h_{j+1}} +\frac{6 y_{j+1}}{h_{j+1} (h_j+h_{j+1})}
\end{equation}
where 
\begin{equation}
\lambda_j = \frac{h_{j+1}}{h_j+h_{j+1}}
\end{equation}
Using two end conditions we have $K+1$ unknowns and $K+1$ equations and we can solve the linear equation system for $M_{j}$. Using the $M_0=\pi_0 M_1$ and $M_K=\pi_K M_{K-1}$ end conditions we can write 
\begin{equation}
\underbrace{\Lambda}_{(K+1) \times (K+1)}=\left[
\begin{tabular}{ccccccc}
2 & -2 $\pi_0$ & 0 & $\ldots$ & 0&0&0 \\
1-$\lambda_1$ & 2 &$ \lambda_1 $ & $\ldots$ & 0&0&0 \\
0 & 1-$\lambda_2$ & 2 & $\ldots$ & 0&0&0 \\
$\vdots$  &   $\vdots$  & $\vdots$  & & $\vdots$  &$\vdots$  &$\vdots$   \\
 0&0&0& $\ldots$ & 2&$\lambda_{K-2}$ &0 \\
 0&0&0& $\ldots$ & 1-$\lambda_{K-1}$&2&$\lambda_{K-1}$ \\
 0&0&0& $\ldots$ & 0&-2 $\pi_K$&2 \\
\end{tabular} \right]
\end{equation}

\begin{equation}
%\underbrace{\Theta}_{\text{\begin{tiny}$(K+1)\times(K+1)$\end{tiny}}}
\Theta=\left[
\tabcolsep=0.07cm
\begin{tabular}{ccccccc}
0&0&0 & $\ldots$ & 0&0&0 \\
$\frac{6}{h_1 (h_1+h_2)}$ & -$\frac{6 }{h_1 h_{2}}$  &$\frac{6}{h_2 (h_1+h_2)}$ & $\ldots$ & 0&0&0 \\
0 & $\frac{6}{h_2 (h_2+h_3)}$ & -$\frac{6 }{h_2 h_{3}}$   & $\ldots$ & 0&0&0 \\
$\vdots$  &   $\vdots$  & $\vdots$  & & $\vdots$  &$\vdots$  &$\vdots$   \\
 0&0&0& $\ldots$ & -$\frac{6 }{h_{K-2} h_{K-1}}$ &$\frac{6}{h_{K-1} (h_{K-2}+h_{K-1})}$ &0 \\
 0&0&0& $\ldots$ & $\frac{6}{h_{K-1} (h_{K-1}+h_{K})}$ &-$\frac{6 }{h_{K-1} h_{K}}$ &$\frac{6}{h_{K} (h_{K-1}+h_{K})}$ \\
 0&0&0& $\ldots$ &0&0&0 \\
\end{tabular} \right] \nonumber
\end{equation}

\begin{equation}
\underbrace{m}_{(K+1) \times 1}=\left[
\begin{tabular}{c}
$M_0$ \\
$M_1$ \\
$\vdots$ \\
$M_{K-1}$ \\
$M_{K}$ \\
\end{tabular} \right]
\end{equation}

\begin{equation}
\underbrace{y}_{(K+1) \times 1}=\left[
\begin{tabular}{c}
$y_0$ \\
$y_1$ \\
$\vdots$ \\
$y_{K-1}$ \\
$y_{K}$ \\
\end{tabular} \right]
\end{equation}
The linear equation system is given by 
\begin{equation}
\Lambda m = \theta y 
\end{equation}
and the solution is 
\begin{equation}
m =\Lambda^{-1}  \Theta y
\label{c_eq:M} 
\end{equation}
Using this result and equation (\ref{c_eq:spline}) we can calculate 
\begin{equation}
\underbrace{S_{\Delta}(\xi)}_{N\times 1}=\left[
\begin{tabular}{c}
$S_{\Delta}(\xi_1)$ \\
$S_{\Delta}(\xi_2)$ \\
$\vdots$ \\
$S_{\Delta}(\xi_{N-1})$ \\
$S_{\Delta}(\xi_N)$ \\
\end{tabular} \right]
\end{equation}
Lets denote $P$ the $N \times (K+1)$ matrix where $i$th row $i=1, \ldots , N1$ given that $k_{j-1} \leq \xi \leq k_{j}$ can be written as 
\begin{equation}
\underbrace{p_i }_{1 \times (K+1)}= \left[\underbrace{0 , \ldots, 0}_{\text{first $j-2$ }}, \frac{k_j - \xi_i}{6 h_j} \left[(k_j - \xi_i)^2 - h^{2}_j  \right],  \frac{\xi_i -k_{j-1} }{6 h_j} \left[(\xi_i -k_{j-1})^2 - h^{2}_j  \right],\underbrace{0 , \ldots, 0}_{\text{last $K+1-j$ }} \right] \nonumber
\end{equation} 
Moreover denote $Q$ the $N \times (K+1)$ matrix where $i$th row $i=1, \ldots , N1$ given that $k_{j-1} \leq \xi \leq k_{j}$ can be written as 
\begin{equation}
\underbrace{q_i }_{1 \times (K+1)}= \left[\underbrace{0 , \ldots, 0}_{\text{first $j-2$ }}, \frac{k_j - \xi_i}{ h_j} ,  \frac{\xi_i -k_{j-1} }{ h_j} ,\underbrace{0 , \ldots, 0}_{\text{last $K+1-j$ }} \right] 
\end{equation} 
Now using (\ref{c_eq:spline}) and (\ref{c_eq:M}) we get 
\begin{equation}
S_{\Delta}(\xi)=Pm+Qy=P\Lambda^{-1}  \Theta y + Qy = (P\Lambda^{-1}  \Theta + Q)y=\underbrace{W}_{N\times (K+1)} \underbrace{y}_{ (K+1)\times 1}
\end{equation}
where 
\begin{equation}
W =P\Lambda^{-1}  \Theta + Q
\end{equation}



In practical situations we might only know the knots but we don't know we observe the spline values with error. In this case we have 
\begin{equation}
s=S_{\Delta}(\xi)+\varepsilon = Wy+\varepsilon,
\end{equation} 
where 
\begin{equation}
\underbrace{s}_{N\times 1}=\left[
\begin{tabular}{c}
$s_1$ \\
$s_2$ \\
$\vdots$ \\
$s_{N-1}$ \\
$s_{N}$ \\
\end{tabular} \right]
\end{equation}
and 
\begin{equation}
\underbrace{\varepsilon}_{N\times 1}=\left[
\begin{tabular}{c}
$\varepsilon_1$ \\
$\varepsilon_2$ \\
$\vdots$ \\
$\varepsilon_{N-1}$ \\
$\varepsilon_{N}$ \\
\end{tabular} \right]
\end{equation}
with
\begin{equation}
\text{E}(\varepsilon)=0 \quad \text{and} \quad E(\varepsilon \varepsilon')=\sigma^2 I
\end{equation}
Notice that after fixing the knots we only have to estimate the value of the spline at he knots and this determines the whole shape of the spline. We cab do this by simple OLS
\begin{equation}
\hat{y}=(W^{\top} W)^{-1}W^{\top}s
\end{equation}
For identification reasons we want
\begin{equation}
\sum \limits_{j : \text{unique} \xi_j} S_{\Delta}(\xi_j )=\sum \limits_{j : \text{unique} \xi_j} w_j y= w^{*}y= 0
\end{equation}
 where $w_i$ is the $i$th row of $W$ and 
 \begin{equation}
  \underbrace{w^{*}}_{1 \times (K+1)}=\sum \limits_{j : \text{unique} \xi_j} w_j
 \end{equation}
The restriction can be enforced by one of the elements of $y$. This ensures that $E(s_t)=0$ so $s_t$ and $\mu_{h}$ can be identified.  If we drop $y_K$ we can substitute 
\begin{equation}
y_K = - \sum \limits_{i=0}^{K-1} (w^{*}_{i}/w^{*}_{K})y_i
\end{equation}
where $w^{*}_{i}$ is the $i$th element of $w^{*}$. Substituting this into
\begin{eqnarray}
\sum \limits_{j : \text{unique} \xi_j} S_{\Delta}(\xi_j )&=&\sum \limits_{j : \text{unique} \xi_j} w_j y= \sum \limits_{j : \text{unique} \xi_j} \sum_{i=0}^{K} w_{ji}y_{i}  = \sum \limits_{j : \text{unique} \xi_j} \sum_{i=0}^{K-1} w_{ji}y_{i} -w_{jK} \sum \limits_{i=0}^{K-1} (w^{*}_{i}/w^{*}_{K})y_i \nonumber \\
&=&\sum \limits_{j : \text{unique} \xi_j} \sum_{i=0}^{K-1} (w_{ji}- w_{jK}w^{*}_{i}/w^{*}_{K})y_{i} =  \sum_{i=0}^{K-1} \sum \limits_{j : \text{unique} \xi_j} (w_{ji}- w_{jK}w^{*}_{i}/w^{*}_{K})y_{i}  \nonumber \\
&=&  \sum_{i=0}^{K-1}  (w^{*}_{i}- w^{*}_{K}w^{*}_{i}/w^{*}_{K})y_{i}= \sum_{i=0}^{K-1}  (w^{*}_{i}- w^{*}_{i})y_{i} =0
\end{eqnarray}
Lets partition $W$ in the following way 
\begin{equation}
\underbrace{W}_{N\times (K+1)} = [ \underbrace{W_{-K}}_{N\times K} : \underbrace{W_{K}}_{N\times 1} ]
\end{equation}
where $W_{-K}$ is equal to the first $K$ columns of $W$ and $W_K$ is the $K$th column of $W$. Moreover 
\begin{equation}
 \underbrace{w^{*}}_{1 \times (K+1)}=[\underbrace{w^{*}_{-K}}_{1\times K} : \underbrace{w^{*}_{K}}_{1\times 1} ] 
\end{equation}
We can define 
\begin{equation}
\underbrace{\widetilde{W}}_{N\times K}= \underbrace{W_{-K}}_{N\times K} - \frac{1}{w^{*}_{K} } \underbrace{W_K}_{N\times 1}  \underbrace{w^{*}_{-K}  }_{1\times K} 
\end{equation}
and we have
\begin{equation}
s=S_{\Delta}(\xi)+\varepsilon =\underbrace{\widetilde{W}}_{N\times K}\underbrace{\tilde{y}}_{K \times 1}+\varepsilon.
\end{equation}




\section{MCMC estimation of the ordered t-SV model}
\label{c_sec:tapp}

In this section,  the $t$ element vectors $(v_1, \ldots, v_t)$ containing time dependent variables for all time time periods, are denoted by $v$, the variable without a subscript.
\subsection{Generating the parameters $x,\mu_{h}$, $\varphi$, $\sigma^{2}_{\eta}$ (Step 2)} 
Notice that conditional on $C=\left\{c_{t}, t=1, \ldots, T  \right\}$ , $r^{*}_{t}$ we have 
\begin{equation}
2 \log r^{*}_{t}  = \mu+s_t+x_t +\log \lambda_t+ m_{ c_{t}}+\varepsilon_{t} , \quad \varepsilon_{t} \sim \mathcal{N}(0, v^{2}_{c_{t}})
\end{equation}
which implies the following following state space form
\begin{eqnarray}
\tilde{y}_t &=&  \underbrace{\left[ 
\begin{tabular}{ccc}
1 &$w_t$ &1 
\end{tabular}
\right]}_{1 \times (K+2)}  \underbrace{\left[ 
\begin{tabular}{c}
$\mu$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1}  +\varepsilon_{t} , \quad \varepsilon_{t} \sim \mathcal{N}(0, v^{2}_{c_{t}}) \label{c_eq:tssf1}\\ 
\alpha_{t+1}  &=& \underbrace{\left[ 
\begin{tabular}{c}
$\mu$ \\
$\beta $ \\
$x_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1} = \underbrace{\left[ 
\begin{tabular}{ccc}
1 &0& 0 \\
0& $I_{K}$ & 0 \\
0 &0& $\varphi$
\end{tabular}
\right]}_{(K+2)\times(K+2)} 
\underbrace{\left[ 
\begin{tabular}{c}
$\mu$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1} +\underbrace{\left[ 
\begin{tabular}{c}
0 \\
0 \\
$\eta_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1}  ,  \eta_{t+1} \sim \mathcal{N}(0,\sigma^{2}_{\eta}) \label{c_eq:tssf2} 
\end{eqnarray}
where 
\begin{equation}
\underbrace{\left[ 
\begin{tabular}{c}
$\mu$ \\
$\beta$ \\
$x_{1}$  
\end{tabular}
\right]}_{(K+2)\times 1} \sim \mathcal{N} \left( \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{0}$ \\
$\beta_{0}$ \\
0
\end{tabular}
\right]}_{(K+2)\times 1} ,\underbrace{\left[ 
\begin{tabular}{ccc}
$\sigma^{2}_{\mu}$ & 0& 0 \\
0&$\sigma^{2}_{\beta} I_{K}$ &  0 \\
0& 0& $\sigma^{2}_{\eta}/(1-\varphi^2)$
\end{tabular}
\right]}_{(K+2)\times (K+2)}  \right)  ,\label{c_eq:tssf3}
\end{equation}
and 
\begin{eqnarray}
\tilde{y}_t = 
2 \log r^{*}_{t}  - \log \lambda_t- m_{ r_{t1}}
\label{c_eq:tytilde}
\end{eqnarray}
First we draw $\varphi, \sigma^{2}_{\eta}$ from $p(\varphi,\sigma^{2}_{\eta}  | \gamma, \nu, C , \tau, N,z_1 , z_2,s ,y)$. Notice that 
\begin{equation}
p(\varphi,\sigma^{2}_{\eta} | \gamma, \nu, C , \tau, N,z_1 , z_2,s ,y)=p(\varphi,\sigma^{2}_{\eta} | \tilde{y}_t, C, N) \propto p(\tilde{y}_t | \varphi,\sigma^{2}_{\eta},C, N) p( \varphi)p(\sigma^{2}_{\eta}),
\end{equation}
where $\tilde{y}_t$ is defined above in equation (\ref{c_eq:ytilde}). The likelihood can be evaluated using standard Kalman filtering and prediction error decomposition (see e.g, \citet{DurbinKoopman2012}) taking advantage of fact that conditional on the auxiliary variables we have a linear Gaussian state space form given by equation (\ref{c_eq:ssf1}),(\ref{c_eq:ssf2}), (\ref{c_eq:ssf3}) and (\ref{c_eq:ytilde}). We draw from the posterior using an adaptive random walk Metropolis-Hastings step proposed by \citet{RobertsRosenthal2009}. 
Conditional on  $\varphi,\sigma^{2}_{\eta} $ we draw $\mu_{h},s$ and $x$ from $p(\mu_{h},s ,x| \varphi,\sigma^{2}_{\eta} ,\gamma, \nu, C, \tau, N,z_1 , z_2,s ,y)$, which is done simulating from the smoothed state density of the linear Gaussian state space model  given by  (\ref{c_eq:tssf1}),(\ref{c_eq:tssf2}), (\ref{c_eq:tssf3}) and (\ref{c_eq:tytilde}). We use the simulation smoother proposed by \citet{DurbinKoopman2002}.
\subsection{Generating  $\gamma$ (Step 3)}
\begin{equation}
p(\gamma | \nu, \mu ,\varphi,\sigma^{2}_{\eta},x , s, C, y,r^{*}_{t})=p(\gamma | \nu, h, y)
\end{equation}
because given $\nu$, $h$ and $y$, the variables $C,\varphi,\sigma^{2}_{\eta}, r^{*}_{t} $ are redundant. 
\begin{equation}
p(\gamma | \nu, h, y)
\propto p(y|\gamma,\nu ,h) p(\gamma|\nu,h)= p(y|\gamma, \nu, h)p(\gamma)
\end{equation}
as  $\gamma$ is independent from $\nu$ and  $h$. 
\begin{eqnarray}
p(y|\gamma,\nu,h)p(\gamma)&=&\prod \limits_{t=1}^{T} \left\{ \gamma \mathbbm{1}_{\left\{y_t =0 \right\}} + (1-\gamma) \right. \nonumber \\
&\times & \left. \left[\mathcal{T}\left(\frac{y_t+0.5}{\exp(h_t/2)}, \nu\right) -\mathcal{T}\left(\frac{y_t-0.5}{\exp(h_t/2)}, \nu\right)  \right]  \right\} \frac{\gamma^{a-1}(1-\gamma)^{b-1}}{\text{B}(a,b)} \nonumber \\
&\propto& \prod \limits_{t=1}^{T} \left\{ \gamma^{a}(1-\gamma)^{b-1} \mathbbm{1}_{\left\{y_t =0 \right\}} + \gamma^{a-1}(1-\gamma)^{b}  \right. \nonumber \\
&\times& \left.    \left[\mathcal{T}\left(\frac{y_t+0.5}{\exp(h_t/2)}, \nu\right) -\mathcal{T}\left(\frac{y_t-0.5}{\exp(h_t/2)}, \nu\right)  \right]  \right\}, \nonumber 
\end{eqnarray}
where $\mathcal{T}(\cdot ,\nu)$ is the Student's $t$ density function with mean zero scale one and degree of freedom parameter $\nu$. We sample from this posterior using an adaptive random walk Metropolis-Hastings sampler by \citet{RobertsRosenthal2009}.
\subsection{Generating $r^{*}$  }
\begin{equation}
p(r^{*}| \gamma, \nu, \mu ,\varphi,\sigma^{2}_{\eta},x , s, C,\lambda, y)=p(r^{*}| \gamma,h,\lambda, y)= \prod \limits_{t=1}^{T}p(r^{*}_t| \gamma, h_t,\lambda_t ,y_t)
\end{equation}
Using the law of total probability
\begin{eqnarray}
p(r^{*}_t| \gamma, \nu, h_t, y_t)&=&p(r^{*}_t| \gamma, \nu, h_t,\lambda_t, y_t, \text{zero})p(\text{zero}|\gamma, h_t,\lambda_t, y_t)\nonumber \\
&+&p(r^{*}_t| \gamma, h_t,\lambda_t, y_t, \text{non-zero})p(\text{non-zero}|\gamma, h_t,\lambda_t, y_t)
\end{eqnarray}
Where $p(r^{*}_t| \gamma, h_t, \lambda_t, y_t, \text{zero})$ is a normal density with zero mean and variance $\lambda_t \exp(h_t)$  truncated to the interval $[y_t - 0.5, y_t +0.5]$.
If $y_t=0$ then 
\begin{eqnarray}
p(\text{zero}|\gamma, h_t, y_t=0)&=& \frac{p(\text{zero},\gamma, h_t, y_t=0)}{p(\gamma, h_t, y_t=0)}=\frac{p(y_t=0| \text{zero},\gamma, h_t)p(\text{zero}|\gamma, h_t)}{p(y_t=0|\gamma, h_t)}\nonumber \\
&=& \frac{1\times \gamma}{\gamma +(1-\gamma)\left[\Phi\left(\frac{0.5}{\sqrt{\lambda_t}\exp(h_t/2)}\right) -\Phi\left(\frac{-0.5}{\sqrt{\lambda_t}\exp(h_t/2)}, \right)  \right]} 
\end{eqnarray}
If $y_t=k\neq0$ then 
\begin{eqnarray}
p(\text{zero}|\gamma, h_t, y_t=k)&=& \frac{p(\text{zero},\gamma, h_t, y_t=k)}{p(\gamma, h_t, y_t=k)} \nonumber \\
&=&\frac{p(y_t=k| \text{zero},\gamma, h_t)p(\text{zero}|\gamma, h_t)}{p(y_t=k|\gamma, h_t)} =0
\end{eqnarray}
Moreover $p(\text{non-zero}|\gamma ,h_t, y_t)=1-p(\text{zero}|\gamma, h_t, y_t)$.
\subsection{Generating $\nu$ and $\lambda$  }
To sample $\nu$ and $\lambda$ we use the method by   \citet{JohannesStroud2014}. 
We can decompose the posterior density as
\begin{eqnarray}
p(\nu,\lambda | \gamma ,\varphi,\sigma^{2}_{\eta},h, C, y,r^{*})&=&p(\nu,\lambda |  h,  r^{*})=p( \lambda | \nu, h,  r^{*})p(\nu| h,  r^{*})
\end{eqnarray}
Note that we have to following mixture representation
\begin{equation}
r^{*}_t=\exp(h_t/2)\sqrt{\lambda_t}\varepsilon_t \quad \varepsilon_t \sim \mathcal{N}(0,1) \quad \lambda_t \sim \text{IG}(\nu/2,\nu/2)
\end{equation}
which implies
\begin{eqnarray}
p(\nu|h,r^{*})&\propto &\prod \limits_{t=1}^{T}  p\left(\frac{r^{*}_t}{\exp(h_t/2)} \middle| h_t,\nu \right)p(\nu)
\end{eqnarray}
where
\begin{eqnarray}
p\left(\frac{r^{*}_t}{\exp(h_t/2)} \middle| h_t,\nu \right) \sim t_{\nu}(0,1)
\end{eqnarray}
and the prior $\nu \sim \mathcal{DU}(2,128)$
which leads to the posterior
\begin{eqnarray}
p(\nu|h,r^{*})&\propto& \prod \limits_{t=1}^{T}  p\left(\frac{r^{*}_t}{\exp(h_t/2)} \middle| h_t,\nu \right)=\prod \limits_{t=1}^{T}  g_{\nu^{*}}\left(\frac{r^{*}_t}{\exp(h_t/2)}\right) =\prod \limits_{t=1}^{T}  g_{\nu^{*}}\left(w_t\right) 
\end{eqnarray}
where $w_t=r^{*}_t/\exp(h_t/2) $.

To avoid the computationally intense evaluation of these probabilities we can use a Metropolis-Hastings update. We can draw the proposal $\nu^{*}$ from the neighbourhood of the current value $\nu^{(i)}$ using a discrete uniform distribution $ \nu^{*} \sim DU(\nu^{(i)}-\delta ,\nu^{(i)}+\delta )$ and accept with probability
\begin{equation}
\min \left\{1, \frac{ \prod_{t=1}^{T} g_{\nu^{*}}(y_t) }{ \prod_{t=1}^{T}g_{\nu^{(i)}}(y_t) }  \right\}
\end{equation}
 $\delta$ is  chosen such that the acceptance rate is reasonable.
 
\begin{eqnarray}
p(\lambda | \nu,  h,  r^{*})=\prod \limits_{t=1}^{T}  p(\lambda_t | \nu,  h_t,  r^{*}_t)\propto \prod \limits_{t=1}^{T} p( r^{*}_t | \lambda_t,  \nu,  h_t)p(\lambda_t| \nu)
\end{eqnarray}
where 
\begin{equation}
p\left( \frac{r^{*}_t}{\exp(h_t/2)} \middle| \lambda_t,  \nu,  h_t\right) \sim \mathcal{N}(0,\lambda_t)
\end{equation}
\begin{equation}
p(  \lambda_t|  \nu ) \sim \text{IG}(\nu/2,\nu/2)
\end{equation}
\begin{equation}
p(\lambda_t | \nu,  h_t,  r^{*}_t)\sim \text{IG}\left(\frac{\nu+1}{2},\frac{\nu+\left(\frac{r^{*}_t}{\exp(h_t/2)}\right)^{2}}{2}\right)
\end{equation}

\section{MCMC estimation of the dynamic $\Delta$NB model} \label{secapp:mcmcdnb} 

In this section,  the $t$ element vectors $(v_1, \ldots, v_t)$ containing time dependent variables for all time time periods, are denoted by $v$, the variable without a subscript.
\label{c_sec:dnb_appendix}
\subsection{Generating the parameters $x,\mu_{h}$, $\varphi$, $\sigma^{2}_{\eta}$ (Step 2)} 
Notice that conditional on $C=\left\{c_{tj}, t=1, \ldots, T , j=1,\ldots, \min(N_t+1,2) \right\}$ , $\tau$, $N$,$\gamma$ and  $s$ we have 
\begin{equation}
- \log \tau_{t1}  = \log(z_{t1}+z_{t2})+ \mu_{h}+s_t+x_t + m_{ c_{t1}}(1)+ \varepsilon_{t1} , \quad \varepsilon_{t1} \sim \mathcal{N}(0, v^{2}_{c_{t1}}(1))
\end{equation}
and
\begin{equation}
- \log \tau_{t2}  = \log(z_{t1}+z_{t2})+ \mu_{h}+s_t+x_t  + m_{ c_{t2}}(N_t)+ \varepsilon_{t2} , \quad \varepsilon_{t2} \sim \mathcal{N}(0, v^{2}_{c_{t2}}(N_t))
\end{equation}
which implies the following  state space form
\begin{eqnarray}
\underbrace{\tilde{y}_t }_{\min(N_t+1,2)\times 1}&=&  \underbrace{\left[ 
\begin{tabular}{ccc}
1 &$w_t$ &1 \\
 1 &$w_t$ &1
\end{tabular}
\right]}_{\min(N_t+1,2) \times (K+2)}  \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1}  + \underbrace{\varepsilon_{t}}_{\min(N_t+1,2)\times 1} , \quad \varepsilon_{t} \sim \mathcal{N}(0, \text{H}_t)  \label{c_eq:ssf1}\\ 
\alpha_{t+1}  &=& \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1} = \underbrace{\left[ 
\begin{tabular}{ccc}
1 &0& 0 \\
0& $I_{K}$ & 0 \\
0 &0& $\varphi$
\end{tabular}
\right]}_{(K+2)\times(K+2)} 
\underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta $ \\
$x_{t}$
\end{tabular}
\right]}_{(K+2)\times 1} +\underbrace{\left[ 
\begin{tabular}{c}
0 \\
0 \\
$\eta_{t+1}$
\end{tabular}
\right]}_{(K+2)\times 1}  ,   \label{c_eq:ssf2}
\end{eqnarray}
where $\eta_{t+1} \sim \mathcal{N}(0,\sigma^{2}_{\eta})$ and 
\begin{equation}
\underbrace{\left[ 
\begin{tabular}{c}
$\mu_{h}$ \\
$\beta$ \\
$x_{1}$  
\end{tabular}
\right]}_{(K+2)\times 1} \sim \mathcal{N} \left( \underbrace{\left[ 
\begin{tabular}{c}
$\mu_{0}$ \\
$\beta_{0}$ \\
0
\end{tabular}
\right]}_{(K+2)\times 1} ,\underbrace{\left[ 
\begin{tabular}{ccc}
$\sigma^{2}_{\mu}$ & 0& 0 \\
0&$\sigma^{2}_{\beta} I_{K}$ &  0 \\
0& 0& $\sigma^{2}_{eta}/(1-\varphi^2)$
\end{tabular}
\right]}_{(K+2)\times (K+2)}  \right)  
 \label{c_eq:ssf3}
\end{equation}
$ \text{H}_t = \text{diag}( v^{2}_{c_{t1}}(1), v^{2}_{c_{t,2}}(N_t))$ and 
\begin{eqnarray}
\underbrace{\tilde{y}_t}_{\min(N_t+1,2)\times 1} = \left(
\begin{tabular}{c}
$- \log \tau_{t1}- m_{ r_{t1}}(1)-\log(z_{t1}+z_{t2})$ \\
$- \log \tau_{t2}- m_{ r_{t2}}(N_t)-\log(z_{t1}+z_{t2})$ 
\end{tabular} \right)
\label{c_eq:ytilde}
\end{eqnarray}
First we draw $\varphi, \sigma^{2}_{\eta}$ from $p(\varphi,\sigma^{2}_{\eta}  | \gamma, \nu, C , \tau, N,z_1 , z_2,s ,y)$. Notice that 
\begin{equation}
p(\varphi,\sigma^{2}_{\eta} | \gamma, \nu, C , \tau, N,z_1 , z_2,s ,y)=p(\varphi,\sigma^{2}_{\eta} | \tilde{y}_t, C, N) \propto p(\tilde{y}_t | \varphi,\sigma^{2}_{\eta},C, N) p( \varphi)p(\sigma^{2}_{\eta}),
\end{equation}
where $\tilde{y}_t$ is defined above in equation (\ref{c_eq:ytilde}). The likelihood can be evaluated using standard Kalman filtering and prediction error decomposition (see e.g, \citet{DurbinKoopman2012}) taking advantage of fact that conditional on the auxiliary variables we have a linear Gaussian state space form given by equation (\ref{c_eq:ssf1}),(\ref{c_eq:ssf2}), (\ref{c_eq:ssf3}) and (\ref{c_eq:ytilde}). We draw from the posterior using an adaptive random walk Metropolis-Hastings step proposed by \citet{RobertsRosenthal2009}. 
Conditional on  $\varphi,\sigma^{2}_{\eta} $ we draw $\mu_{h},s$ and $x$ from $p(\mu_{h},s ,x| \varphi,\sigma^{2}_{\eta} ,\gamma, \nu, C, \tau, N,z_1 , z_2,s ,y)$, which is done simulating from the smoothed state density of the linear Gaussian state space model  given by  (\ref{c_eq:ssf1}),(\ref{c_eq:ssf2}), (\ref{c_eq:ssf3}) and (\ref{c_eq:ytilde}). We use the simulation smoother proposed by \citet{DurbinKoopman2002}.
\subsection{Generating  $\gamma$ (Step 3)}
\begin{equation}
p(\gamma | \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},x ,C , s,\tau, N,z_1 , z_2 , y)=p(\gamma | \nu, \mu_{h},s ,x ,y)
\end{equation}
because given $\nu$, $\lambda$ and $y$, the variables $C , \tau, N ,z_1 , z_2 $ are redundant. 
\begin{equation}
p(\gamma |\nu, \mu_{h},s ,x ,y)
\propto p(y|\gamma,\nu ,\mu_{h} ,s,x) p(\gamma|\nu,\mu_{h} ,s,x)= p(y|\gamma, \nu, \mu_{h} ,s,x)p(\gamma)
\end{equation}
as  $\gamma$ is independent from $\nu$ and  $\lambda_t=\exp(\mu_{h}+ s_t+x_t)$. 
\begin{eqnarray}
p(y|\gamma,\nu,\mu_{h} ,x)p(\gamma)&=&\prod \limits_{t=1}^{T} \left[ \gamma \mathbbm{1}_{\left\{y_t =0 \right\}} + (1-\gamma)  \left(\frac{\nu}{ \lambda_t+\nu}\right)^{2\nu}\left(\frac{\lambda_t}{ \lambda_t+\nu}\right)^{|y_t |} \frac{\Gamma(\nu+|y_t |) }{\Gamma(\nu)\Gamma(|y_t |)} \right. \nonumber \\
&\times & \left. F\left(\nu+y_t , \nu, y_t+1; \left(\frac{\lambda_t}{ \lambda_t+\nu}\right)^2\right) \right] \frac{\gamma^{a-1}(1-\gamma)^{b-1}}{\text{B}(a,b)} \nonumber \\
&\propto& \prod \limits_{t=1}^{T} \left[ \gamma^{a}(1-\gamma)^{b-1} \mathbbm{1}_{\left\{y_t =0 \right\}} + \gamma^{a-1}(1-\gamma)^{b} \left(\frac{\nu}{ \lambda_t+\nu}\right)^{2\nu}\left(\frac{\lambda_t}{ \lambda_t+\nu}\right)^{|y_t |} \right. \nonumber \\
&\times& \frac{\Gamma(\nu+|y_t |) }{\Gamma(\nu)\Gamma(|y_t |)}  \left. F\left(\nu+y_t , \nu, y_t+1; \left(\frac{\lambda_t}{ \lambda_t+\nu}\right)^2\right)   \right] \nonumber 
\end{eqnarray}
We sample from this posterior using an adaptive random walk Metropolis-Hastings sampler.
%We can carry out an independent MH step to sample from this density using a truncated normal or normal density with mean equal  to the mode of this above distribution and variance equal to the Hessian at the mode.
\subsection{Generating  $ C , \tau, N, z_1 , z_2 , \nu$ (Step 4)} 
 We can decompose the joint posterior of $C, \tau, N ,z_1 , z_2, \nu$ into
\begin{eqnarray}
p(C, \tau, N ,z_1 , z_2, \nu|\gamma,\mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)&=& p(C |\tau, N, z_1 , z_2 \gamma, p, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y) \nonumber \\ 
&\times& p(\tau | N, z_1 , z_2 \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)  \nonumber \\ 
&\times& p( N| z_1 , z_2 \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)\nonumber \\ 
&\times& p(  z_1 , z_2 | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)\nonumber \\ 
&\times&   p(\nu | \gamma, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)
\end{eqnarray} 

\subsubsection*{Generating $\nu$ (Step 4a) }
Note that
\begin{eqnarray}
p(\nu | \gamma, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y)&=& p(\nu | \gamma, \lambda,y) \nonumber \\
&\propto& p(\nu , \gamma, \lambda,y)\nonumber \\
&=& p(y|\gamma, \lambda,\nu )p( \lambda|\gamma,\nu )p( \gamma|\nu )p( \nu )
\nonumber \\
&=&  p(y|\gamma, \lambda,\nu )p( \lambda )p( \gamma )p( \nu )
 \nonumber \\
&\propto& p(y|\gamma, \lambda,\nu )p( \nu )
\end{eqnarray}
where $p(y|\gamma, \lambda,\nu )$ is a product of zero inflated $\Delta$NB probability mass functions. 
%We can draw $\nu$ using a Laplace approximation or an adaptive random walk Metropolis-Hasting procedure.

We draw $\nu$  using a discrete uniform prior $ \nu \sim DU(2,128)$ and a random walk proposal in the following fashion as suggested by \citet{JohannesStroud2014} for degree of freedom parameter of a t density. We can write the posterior as a multinomial distribution 
$p(\nu |  \mu_{h} ,x , z_1 , z_2) \sim M(\pi^{*}_2, \ldots, \pi^{*}_{128}) $ with probabilities
\begin{equation}
\pi^{*}_{\nu} \propto  \prod \limits_{t=1}^{T}
\left[\gamma\mathbb{I}_{\left\{ y_t =0\right\}}  + (1-\gamma)f_{\Delta\text{NB}}(y_t; \lambda_t, \nu ) \right]=  \prod \limits_{t=1}^{T} g_{\nu}(y_t) 
\end{equation}
 
To avoid the computationally intense evaluation of these probabilities we can use a Metropolis-Hastings update. We can draw the proposal $\nu^{*}$ from the neighbourhood of the current value $\nu^{(i)}$ using a discrete uniform distribution $ \nu^{*} \sim DU(\nu^{(i)}-\delta ,\nu^{(i)}+\delta )$ and accept with probability
\begin{equation}
\min \left\{1, \frac{ \prod_{t=1}^{T} g_{\nu^{*}}(y_t) }{ \prod_{t=1}^{T}g_{\nu^{(i)}}(y_t) }  \right\}
\end{equation}
 $\delta$ is  chosen such that the acceptance rate is reasonable.

\subsubsection*{Generating $z_1 , z_2$ (Step 4b) }

Notice that $ z_1 , z_2$ are independent given $ \gamma , \mu_{h},s, x, y$.
\begin{eqnarray}
p(  z_1 , z_2 | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,y) = \prod \limits_{t=1}^{T} p(  z_{t1} , z_{t2} | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ,y_t)
\end{eqnarray}

\begin{eqnarray}
p(  z_{t1} , z_{t2} | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ,y_t) &\propto& p(  z_{t1} , z_{t2}, \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ,y_t) \nonumber \\ &=& p( y_t |  z_{t1}, z_{t2}, \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ) \nonumber \\
&\times& p(   z_{t1} , z_{t2} | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ) 
\end{eqnarray}
\begin{eqnarray}
p(  z_{t1} , z_{t2} | \gamma, \nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s_t,x_t ,y_t)  \propto g(z_{t1} ,z_{t2})\frac{\nu^\nu z_{t1}^{\nu} e^{-\nu z_{t1}}}{ \Gamma(\nu)}
\frac{\nu^\nu z_{t2}^{\nu} e^{-\nu z_{t2}}}{ \Gamma(\nu)}
\end{eqnarray}
where
\begin{equation}
g(z_{t1} ,z_{t2})= \left[ \gamma \mathbbm{1}_{\left\{y_t =0 \right\}} + (1-\gamma) \exp \left[-\lambda_t (z_{t1} + z_{t2}) \right] \left(\frac{ z_{t1} }{  z_{t2} } \right)^{\frac{y_t}{2}} I_{|y_t |} ( 2 \lambda_t \sqrt{ z_{t1}z_{t2}}) \right]
\end{equation}
with $\lambda_t=\exp(\mu_{h}+ s_t+x_t)$.
We can carry out an independent  MH step by sampling $z^{*}_{1t} ,z^{*}_{2t}$ from $\text{Ga}( \lambda_t , \nu) $
and accept it with probability 
\begin{equation}
\min \left\{ \frac{g(z^{*}_{1t} ,z^{*}_{2t})}{g(z_{t1} ,z_{t2})} ,1 \right\}
\end{equation}


\subsubsection*{Generating $N$ (Step 4c) }

Note that condition on  on $z_{t1}$ , $z_{t2}$
and the intensity $\lambda_t$  the $N_t$ are independent over time, hence
\begin{equation}
p( N| \gamma, \nu,\mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x ,z_{1} ,z_{2},y)= \prod \limits_{t=1}^{T}p( N_t | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t ) .
\end{equation}
For a given $t$ we can draw $N_t $ from a discrete distribution with  
 \begin{eqnarray}
p( N_t  | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t ) &=& \frac{p( N_t, y_t  | \gamma, \lambda_t ,z_{t1} ,z_{t2})}{p(y_t | \gamma, \lambda_t ,z_{t1} ,z_{t2}) }  \nonumber \\
 &=& \frac{p( y_t | N_t, \gamma, \lambda_t ,z_{t1} ,z_{t2})p( N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) }{p(y_t|  \gamma, \lambda_t ,z_{t1} ,z_{t2})} \nonumber \\
 &=& \left[ \gamma \mathbbm{1}_{\left\{y_t=0 \right\}} +(1-\gamma) p\left( y_t |N_t , \lambda_t ,z_{t1} ,z_{t2}\right) \right] \nonumber \\
 &\times &\frac{p( N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) }{p(y_t|  \gamma, \lambda_t ,z_{t1} ,z_{t2})} 
 \label{c_eq:neq2}
 \end{eqnarray}
 The denominator in equation (\ref{c_eq:neq2}) is a Skellam  distribution  with intensity $\lambda_{t} z_{t1}$ and$\lambda_{t} z_{t2}$. We can calculate  probability 
\begin{equation}
 p\left( y_t |N_t , \lambda_t ,z_{t1} ,z_{t2}\right) 
\end{equation} 
using the results from equation (\ref{c_eq:gammapo})
condition on $\lambda_t$, $z_{t1}$ and $z_{t2}$, $y_t$ is distributed as a marked Poisson process with marks given by 
\begin{equation}
M_i =\begin{cases}
    1, & \text{with} \quad P(M_i = 1)= \frac{z_{t1} }{z_{t1}+z_{t2} } \\
    -1, & \text{with} \quad P(M_i = -1)= \frac{z_{t2} }{z_{t1}+z_{t2} } 
  \end{cases},
\end{equation}
which implies that we can represent $y_t$ as $\sum \limits_{i=0}^{N_t} M_i $.  

% with a tree structure and the binomial distribution. $\sum \limits_{i=1}^{n} M_i $ has a binomial distribution with $n$ trails, $(n+k)/2$ successes and $p=0.5$ success rate. Note that even $k$ can only happen in even number of trails and odd $k$ can only happen in odd number of trails. 
\begin{equation}
p\left( y_t | N_t, \lambda_t, z_{t1} ,z_{t2}\right) =\begin{cases}
    0\quad,\quad  \text{if}  \quad  y_t > N_t \quad \text{or}\quad |y_t| \bmod 2\neq |N_t| \bmod 2 &    \\
    \displaystyle \binom{N_t}{\frac{N_t+y_t}{2}} \left(\frac{z_{t1} }{z_{t1}+z_{t2}} \right)^{\frac{N_t+y_t}{2}}\left(\frac{z_{t2} }{z_{t1}+z_{t2}} \right)^{\frac{N_t-y_t}{2}} , & \text{otherwise}
  \end{cases}
\end{equation}

Conditional  on $z_{t1}$ , $z_{t2}$
and  $\lambda_t$, $N_t$ is a realization of a Poisson process on $[0,1]$ with intensity $(z_{t1}+z_{t2})\lambda_t$, hence the probability $p(  N_t| \gamma, \lambda_t ,z_{t1} ,z_{t2}) $  is a Poisson random variable with intensity equal to $\lambda_t ( z_{t1}+z_{t2})$.
We can draw $N_t $ parallel over $t= 1, \ldots, T$ by drawing a uniform random variable $u_t \sim U[0,1]$ and 
\begin{equation}
N_t = \min  \left\{ n : u_t \leq \sum \limits_{i=0}^{n} p( i  | \gamma, \lambda_t ,z_{t1} ,z_{t2}, y_t )  \right\}
\end{equation}



\subsubsection*{Generating $\tau $ (Step 4d) }
Notice that $p( \tau |N, z_1 , z_2,\gamma,\nu,\mu_{h} ,\varphi,\sigma^{2}_{\eta},x,y )= p(\tau|N, \mu_{h},z_1, z_2,s, x)$. Moreover
\begin{eqnarray}
p(\tau| \mu_{h},z_1, z_2,s, x)&=& \prod \limits_{t=1}^{T} p(\tau_{1t}, \tau_{2t}| N_t,\mu_{h},z_{t1}, z_{t2}, s_t,x_t )\nonumber \\
&=&  \prod \limits_{t=1}^{T} p(\tau_{1t}|  \tau_{2t}, N_t, \mu_{h},z_{t1}, z_{t2},s_t, x_t )p(\tau_{2t}| N_t, \mu_{h},z_{t1}, z_{t2},s_t, x_t ) \nonumber
\end{eqnarray}
where we can sample from $p(\tau_{2t}| N_t, \mu_{h},z_{t1}, z_{t2},s_t, x_t )$ using the fact that conditionally on $N_t$ the arrival time $\tau_{2t}$ of the $N_t$th jump is the maximum of $N_t$ uniform random variables and it has a $Beta(N_t,1)$ distribution. The arrival time of the $(N_t +1)$th jump after 1 is exponentially distributed with intensity $\lambda_t (z_{t1}+z_{t2})$, hence
\begin{equation}
\tau_{1t} = 1+\xi_t-\tau_{2t} \quad \xi_t \sim \text{Exp}(\lambda_t (z_{t1}+z_{t2}))
\end{equation}


\subsubsection*{Generating $C$ (Step 4e) }
Notice that 
\begin{equation}
p(C | \tau, N,z_1 , z_2,\gamma,\nu, \mu_{h} ,\varphi,\sigma^{2}_{\eta},s,x,y )= p(C|\tau, N,z_1 , z_2,\nu, s,x)
\end{equation}
Moreover
\begin{equation}
 p(C|\tau, N,z_1 , z_2,\nu, s,x)= \prod \limits_{t=1}^{T} \prod \limits_{j=1}^{\min(N_t+1,2)} p(r_{tj}| \tau_{t}, N_t,\mu_{h},z_{t1}, z_{t2},s_t, x_t )
\end{equation}
Sample $c_{t1}$ from the following discrete distribution 
\begin{equation}
p(c_{t1}| \tau_{t}, N_t,\mu_{h},z_{t1}, z_{t2},s_t, x_t )\propto w_{k}(1) \varphi ( - \log \tau_{1t} - \log [\lambda_t (z_{t1}+ z_{t2}) ] , m_{k}(1), v_{k}^2(1) )
\end{equation}
where $k=1,\ldots, C(1)$
If $N_t >0$ then draw $r_{t2}$ from the discrete distribution 
\begin{equation}
p(c_{t2}| \tau_{t}, N_t,\mu_{h},z_{t1}, z_{t2},s_t, x_t )\propto w_{k}(N_t) \varphi ( - \log \tau_{1t} - \log [\lambda_t (z_{t1}+ z_{t2}) ] , m_{k}(N_t), v_{k}^2(N_t) ) \nonumber
\end{equation}
for $k=1,\ldots, C(N_t)$

These algorithmic details also apply to the dynamic Skellam model.
Illustrations of the resulting posterior distributions of the parameters
are given in Figure \ref{c_pic:dnb_res_pic} for our $\Delta$NB model and
in Figure \ref{c_pic:skellam_res_pic} for the dynamic Skellam model.

\begin{figure}[!htp]
  \centering 
\includegraphics[scale=0.65]{Pictures/0000_Sk_Skdens_all.pdf}
       \caption{ The posterior distribution of the parameters from a dynamic Skellam model based on 20000 observations and 100000 iterations from which 20000 used as a burn in sample. Each picture shows the histogram of the posterior draws the kernel density estimate of the posterior distribution, the HPD region and the posterior mean. The true parameters are $\mu=-1.7$, $\varphi=0.97$ , $\sigma=0.02$, $\gamma=0.001$ }
 \label{c_pic:skellam_res_pic}
 \end{figure} 



\section{Log returns versus price changes}

Stock prices can be quoted as a multiple of the tick size. As a consequence prices are defined on a discrete grid, where the grid points are a tick size distance away from each other. We can write the prices at time $t_j$ as 
\begin{equation}
p(t_j) =  n(t_j)g
\end{equation}
where $g$ is the tick size which can be the function of the price on some exchanges and $n(t_j)$ is a natural number, denoting the location of the price on the grid.
Modelling trade by trade returns can pose difficulty as  the effect of price discreteness on a few seconds frequency is pronounced compared to lower frequencies such as one  hour or one day. As described in \citet{Munnix2010}, the problem is that the return distribution is a mixture of return distributions $r^i $, which correspond to fix price changes $ig$
\begin{equation}
r^{i}=\left\{\frac{p(t_j)-p(t_{j-1})}{p(t_{j-1})} \mathrel{\bigg|} p(t_j)-p(t_{j-1})=\left[n(t_j) - n(t_{j-1})\right]g = i(t_j)g=ig \right\}
\end{equation}
where $i(t_j)$ is an integer, which express the price change in terms of ticks. The $r^i$ distributions are on the intervals (for positive $i$)
\begin{equation}
\left[ \frac{ig}{\max p^{i}} , \frac{ig}{\min p^{i}} \right],
\end{equation}
where 
\begin{equation}
p^{i}=\left\{p(t_{j-1}) \mathrel{\bigg|} p(t_j)-p(t_{j-1})=\left[n(t_j) - n(t_{j-1})\right]g = i(t_j)g=ig \right\}
\end{equation}
These interval and the center of the intervals $c_i$ can be approximated by 
\begin{equation}
\left[ \frac{ig}{\bar p} , \frac{ig}{ \munderbar{p}} \right],
\end{equation}
and 
\begin{equation}
c_i \approx \frac{ig}{ 2}\left( \frac{1}{ \munderbar p} -\frac{1}{\bar p}  \right)
\end{equation}
as $ \max p^{i} \approx \bar p $  and $ \min p^{i} \approx \munderbar p $  for $i$ close to 0.

First, note that the intervals corresponding to zero price change and one tick changes are always non-overlapping. 
Secondly,  the center of the intervals are approximately equally spaced, however the intervals for higher absolute value changes are wider, which means that the intervals are getting more and more overlapping as $|i|$ is increasing. Thirdly, the intervals are less overlapping when the price is lower, the volatility is higher or the tick size is bigger. Figure \ref{c_pic:logreturn_pic} shows the empirical trade by trade return distribution of several stocks from the New York Stock Exchange (NYSE).

\begin{figure}[H]
  \centering 
\includegraphics[scale=0.6]{Pictures/hist_logreturn_200810.pdf}
       \caption{ Empirical distribution of the tick by tick log returns during October 2008 for Alcoa (AA), Ford (F), International Business Machines (IBM),JP Morgan (JPM), Coca-Cola (KO) and Xerox (X) }
 \label{c_pic:logreturn_pic}
 \end{figure} 

\begin{figure}[H]
  \centering 
\includegraphics[scale=0.6]{Pictures/hist_tickreturn_200810.pdf}
       \caption{ Empirical distribution of the tick returns along with fitted Skellam density during October 2008 for Alcoa (AA), Ford (F), International Business Machines (IBM),JP Morgan (JPM), Coca-Cola (KO) and Xerox (X) }
 \label{c_pic:tickreturn_pic}
 \end{figure}  
 
\section*{Data cleaning}
\label{c_sec:dataclean}
\begin{sidewaystable}{H}
%\begin{small}
\begin{scriptsize}
%\begin{tiny}
\begin{center}
\caption{Summary of the cleaning and aggregation procedure on the data from 3rd to 10th October 2008 for  Alcoa (AA), Coca-Cola (KO) International Business Machines (IBM), J.P. Morgan (JPM), Ford (F), Xerox (XRX from the NYSE.}
\input{Tables/data200810_table.tex} 
\label{c_table:data2008}
\end{center}
\begin{center}
\caption{Summary of the cleaning and aggregation procedure on the data from  23rd to 30th April 2010 for  Alcoa (AA), Coca-Cola (KO) International Business Machines (IBM), J.P. Morgan (JPM), Ford (F), Xerox (XRX from the NYSE.}
\input{Tables/data201004_table.tex}
\label{c_table:data2010}
\end{center}
\end{scriptsize}
%\end{small}
%\end{tiny}
\end{sidewaystable} 
 



 
 \end{document}

